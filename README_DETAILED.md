Sniffer Project Source Code Analysis
CliConfig.java
Summary: The CliConfig class handles parsing of command-line arguments for the sniffer application. It collects interface and capture parameters (like interface name, buffer size, snap length, timeout, BPF filter, etc.) into easy-to-use fields. This allows the rest of the application to configure packet capture based on user input without duplicating parsing logic.
Key Components: - Fields: iface, bpf (filter expression), bufMb, snap, timeoutMs, verbose, headers – these store capture interface, filter, buffer size in MB, snapshot length, capture timeout, and flags for verbose output or printing full headers. - Constructor: Parses an array of String arguments (expected in the form key=value) and populates the fields. Default values are provided if certain keys are absent (e.g. default snap length 65535, timeout 1ms, etc.). It uses a simple switch on the argument key to assign each field.
Architecture & Usage: This class is a simple configuration holder. It’s used by the CLI entry points (like CaptureRunner.main) to interpret user parameters. After parsing, other components (like CaptureLoop or CaptureMain) read these fields to set up capture accordingly. This keeps configuration logic separate from capture logic.
Performance Considerations: CliConfig performs minimal work (string splitting and parsing of integers/booleans). This happens only at startup, so performance is not an issue here. The values are stored in primitive types or Strings for efficiency.
Extensibility: To support new command-line options, the constructor’s parsing switch can be extended with new cases and corresponding fields. The design is straightforward, though it lacks validation (e.g., ensuring required fields are present or within ranges). Future improvements might include more robust error handling or using a standardized CLI library, but the current approach is simple and effective for known parameters.
External Libraries/Dependencies: None. It uses only core Java (string handling) to parse arguments.
CaptureRunner.java
Summary: The CaptureRunner class is a simple command-line runner that ties together configuration and live capture in one process. It parses CLI arguments into a CliConfig, opens a packet capture on the specified interface, and runs an integrated capture loop. It’s primarily a convenience for interactive use or testing – capturing packets and printing HTTP information to the console.
Major Steps in main(): - Configuration: It creates a CliConfig from args. If no interface is specified, it prints a usage message and exits, since iface is required. - Pcap Initialization: It instantiates a JnrPcapAdapter (the libpcap adapter) and logs the libpcap version. Then it calls pcap.openLive(...) with parameters from config to get a PcapHandle. This opens the network interface for capturing with the given snap length, buffer size, timeout, etc. - Running Capture Loop: It creates a new CaptureLoop instance, passing the open Pcap and a PacketSink. Here, PacketSink is implemented as an inner class StdoutSink that writes output to standard output. This sink’s onHttpLine method prints HTTP request/response lines, and onUdp prints UDP packet info. - Starting Capture: It calls loop.runForever(...) with interface and capture parameters. This enters an indefinite loop to fetch packets and process them.
Architecture & Interaction: CaptureRunner essentially glues together the lower-level components for a one-step capture: - It uses CaptureLoop (from the domain package) to process packets and assemble HTTP on the fly, using the provided PacketSink to output results. - The design demonstrates a single-process mode: capture + assemble + output happening in one JVM, rather than the multi-process pipeline described in the docs. It’s useful for quick tests or development.
Performance Considerations: This runner runs everything on one thread (the capture loop thread). It prints results synchronously, which could become a bottleneck at high traffic rates. Writing to stdout and assembling HTTP in real-time may not keep up with a high volume interface. The integrated approach may drop packets if processing (especially printing) can’t keep pace with capture. However, it’s acceptable for moderate traffic or debugging. The code does allocate and log per packet (e.g., formatting strings for output), but this is unavoidable for human-readable output.
Extensibility: Future developers can modify StdoutSink to write to a file or another system instead of console, or integrate additional processing in the loop. New CLI options (for example, to toggle different output formats) would be parsed via CliConfig and respected in the sink or loop. If needed, this class could be expanded to support offline pcap files (though currently it’s live only) by adding an option and using a different Pcap source.
External Libraries: It relies on the JnrPcapAdapter to interface with libpcap (via the JNR FFI library). Aside from JNR for capture, it uses only core Java for I/O and printing.
CaptureConfig.java
Summary: CaptureConfig is a configuration helper class used by the standalone capture program (CaptureMain). It parses command-line arguments (similar to CliConfig) but tailored for the capture-to-file pipeline. It includes parameters for output directories and file rotation in addition to interface and libpcap settings.
Key Fields: - Interface & BPF: iface (network interface), bpf (filter expression). - Capture Params: snap length, bufBytes (buffer size in bytes), timeoutMs, promisc (promiscuous mode on/off), immediate (immediate mode on/off). - Output Params: outDir (Path to output directory for capture files), fileBase (base name for output files), rollMiB (rotate file after this many MiB).
Parsing (fromArgs method): - It takes an array of args in the form key=value and populates a map. Default values are provided for many settings (e.g., "iface" defaults to "eth0", buffer MB default 256, etc.). - It converts units as needed (e.g., bufmb is converted to bytes by multiplying by 1024^2). - Boolean flags like promisc and immediate are parsed via Boolean.parseBoolean, defaulting to true. - The output directory defaults to "./cap-out" and file base to "capture" if not provided. Rotation size defaults might be included (e.g., rollMiB default could be set, say 100 MiB or user-provided).
Usage in Application: CaptureMain uses CaptureConfig.fromArgs(args) to get a configured instance. This separates argument parsing from the capture logic. The config values are then used to open the Pcap handle and instantiate the segment writer (SegmentIO).
Performance & Design: Like CliConfig, this runs once at startup. It uses simple string parsing and defaulting, which is low overhead. The class is package-private (final class CaptureConfig) and used only in the capture package, indicating it’s an internal detail of CaptureMain.
Extensibility: To add new capture-related parameters (e.g., a flag to disable ACK filtering or adjust thread counts), one would extend this parsing. As it stands, it covers key parameters for libpcap and file output. It could be improved with more robust parsing or help text on invalid input, but it’s sufficient for the intended use. The values are all public (package-private final fields), so other classes can easily access them.
External Libraries: None directly – it uses only core Java classes. (It indirectly influences use of JNR via CaptureMain, but CaptureConfig itself does not use external libraries.)
CaptureMain.java
Summary: CaptureMain is the entry point for the first stage of the pipeline – capturing raw TCP segments from a network interface and writing them to disk. It opens a live capture on the specified interface and enters a loop, where each captured packet is decoded and, if it’s a TCP segment with payload, written to a binary segment file.
Workflow in main: 1. Initialize Config: It builds a CaptureConfig from command-line args. Key settings include the network interface, BPF filter (if any), and output directory for captured data. It prints a log line with the effective configuration (interface, buffer, snaplen, timeout, BPF, output path, and file rotation size). 2. Prepare Output: It creates the output directory (if not existing) for segment files. 3. Open Capture Device: Using the Pcap SPI, it obtains a PcapHandle. Specifically, it creates a JnrPcapAdapter (libpcap via JNR) and calls an openLive with parameters (iface, snap length, promiscuous, timeout, buffer size, immediate mode). If a BPF filter string was given, it installs it via h.setFilter(...) on the handle. 4. Capture Loop: Within a try-with-resources (to ensure the handle and writer are closed properly), it instantiates SegmentIO.Writer to append segments to file (with automatic file rotation). Then it enters a loop: - It repeatedly calls ph.next(callback). The callback is a lambda handling each packet. - For each packet, it decodes it via TcpDecoder.decode(data, caplen). If decoding returns null (meaning the packet is not a TCP/IPv4/IPv6 packet or couldn’t be decoded), it ignores the packet. - If decoded and identified as TCP, it checks for “pure ACK” packets – i.e., packets with no payload and only the ACK flag. Those are skipped (not written) to reduce noise and file size (pure ACKs carry no HTTP data). This is determined by d.payloadLen <= 0 && (d.flags & ~SegmentRecord.ACK) == 0. - If not a pure ACK, it extracts the payload bytes. The code uses a helper slice(byte[] data, int offset, int length) to safely copy the payload portion of the packet. (This handles edge cases: it clamps offsets/length to array bounds and returns an empty array if no payload.) - It then constructs a SegmentRecord object and populates it with decoded fields: timestamp (microseconds), source IP:port, dest IP:port, sequence number, TCP flags, and the payload bytes. This uses SegmentRecord.fill(...) for convenience. - The segment record is appended to the output via writer.append(seg). If an IOException occurs during write, it logs an error to stderr but continues capturing. - The loop continues as long as ph.next(...) returns true (in live capture, it returns true for each packet or timeout; it would return false only if an offline capture file ended). 5. Shutdown: On exiting the loop (which might be triggered by external interruption or error), it flushes the writer and exits. The try-with-resources ensures the file is flushed and closed and the pcap handle is closed.
Interactions and Architecture: CaptureMain is a self-contained capture process. It heavily relies on: - TcpDecoder (from sniffer.capture) to interpret raw packet bytes into structured TCP segment info. - SegmentIO.Writer (from sniffer.pipe) to efficiently record segments to disk. This writer handles file buffering and rotation logic. - PcapHandle (from sniffer.spi) for packet capture. The actual implementation is provided by the JnrPcapAdapter, which interfaces with native libpcap.
The design cleanly separates concerns: decoding logic is in TcpDecoder, file I/O in SegmentIO, so CaptureMain mainly orchestrates and ties them together.
Performance Considerations: This capture loop is designed for high throughput: - It uses a large ring buffer in libpcap (configurable via bufBytes) and immediate mode to receive packets with minimal latency. - The decoding is done in Java but is lightweight (just parsing headers and copying payload). - By filtering out pure ACKs, it reduces the number of writes significantly in HTTP flows (ACKs often make up a large fraction of TCP packets). - Writing to disk is sequential and buffered (1 MiB buffer in SegmentIO.Writer), which is efficient on modern SSDs. Rotation by size/time ensures files remain manageable. - The main potential bottleneck is the single-threaded nature: one thread decodes and writes all packets. If network traffic is extremely high (e.g., 10 Gbps line rate), this Java process may become CPU-bound. However, using large buffers and skipping irrelevant packets helps. The design anticipates sustainable 10 Gbps capture by focusing on sequential I/O and minimal per-packet overhead. - Memory usage per packet is minimal: a new SegmentRecord and a byte array for payload are allocated for each kept packet. The payload byte array size equals the TCP payload length (which might be up to snap length). Frequent allocations could strain the GC at very high packet rates, but it’s a trade-off for simplicity. (If needed, a byte buffer pool or reuse could be considered in future.)
Extensibility: - Protocol handling: Currently it only targets TCP (IPv4/IPv6). If future requirements include UDP or other protocols, developers could integrate logic either in the capture loop or via extending TcpDecoder (or a parallel UdpDecoder). The PacketCallback in Pcap allows deciding different actions depending on protocol. - Data sinks: Right now, SegmentIO.Writer writes a proprietary binary format. One could swap in a different sink (e.g., writing to a database or a network stream) by modifying the loop to call another writer. The modular design (decode -> create record -> append to sink) makes this straightforward. - Filtering & logic: Additional filtering could be added (for instance, skipping known non-HTTP ports entirely aside from BPF, or sampling traffic). Also, error handling could be extended to handle disk-full scenarios (currently not explicitly handled). - Multithreading: For extremely high throughput, a future extension might spawn multiple threads to handle decoding/writing, but that would require careful partitioning of traffic (perhaps via RSS queue assignment or similar) to maintain ordering within flows. The current single-thread approach ensures in-order writing of segments as captured.
External Libraries: The key external dependency is JNR (Java Native Runtime) to call libpcap. CaptureMain uses JnrPcapAdapter indirectly; JNR performs the native calls to open devices and capture packets. Aside from JNR, it uses core Java I/O (for file and directory handling) and the Java time library for timestamps in log output.
TcpDecoder.java
Summary: TcpDecoder is a utility class that inspects raw packet bytes and determines if they contain a TCP segment (over IPv4 or IPv6). If so, it extracts essential information needed for reassembly: IP addresses, ports, sequence number, TCP flags, and payload position/length within the packet. This avoids writing a full protocol stack – it’s a lightweight parser to quickly identify TCP payload data.
Functionality: - It defines a static inner class Decoded which holds the parsed fields: source IP (src), source port (sport), destination IP (dst), destination port (dport), sequence number (seq), flags (flags bitmask), and payloadOff/payloadLen to locate the payload in the original packet byte array. - The core method is static Decoded decode(byte[] pkt, int caplen). This method operates as follows: 1. Ethernet Parsing: It assumes the link layer is Ethernet. It checks that the captured packet length is at least 14 bytes (Ethernet header). It reads the EtherType at bytes 12–13. It also handles VLAN tags: if the EtherType is 0x8100 or 0x88A8 (VLAN tagged frames), it skips the 4-byte tag and reads the next EtherType. 2. IP Parsing: If the EtherType corresponds to IPv4 (0x0800): - It ensures at least 20 bytes remain for an IPv4 header. It reads the first byte to get version and IHL. If version is not 4 or header length is invalid, it returns null. - It checks the protocol field; if it’s not 6 (TCP), returns null (i.e., not a TCP packet). - It extracts source and destination IP addresses (calls a helper ipv4(pkt, offset) which formats the 4 bytes into a dotted-quad string). - It computes the IP payload length from the IP header’s total length field, clamped to captured length (caplen minus IP header length). - It advances the offset past the IP header. If EtherType is IPv6 (0x86DD): - Checks minimum 40 bytes for IPv6 header. Reads the “Next Header” field; if not TCP (6), returns null. - Extracts source and destination addresses (the helper ipv6(pkt, offset) converts 16 bytes to a standard colon-separated IPv6 address string). - Determines payload length from the IPv6 header’s payload length field (clamped similarly). - Advances offset past the 40-byte IPv6 header. For any other EtherType, or if any checks fail, it returns null (meaning the packet is not a TCP/IP packet that we handle). 3. TCP Parsing: Now at the start of the TCP header: - Checks at least 20 bytes remain for the basic TCP header. Reads source port and dest port (16-bit each). - Reads sequence number (32-bit) and acknowledges number (but ACK isn’t stored since not needed for payload analysis). - Reads data offset (header length) from the high 4 bits of the byte at offset 12. If the TCP header length is less than 20 or beyond remaining bytes, returns null (malformed). - Reads the flags byte (offset 13) and maps it to a bitmask of FIN/SYN/RST/PSH/ACK using constants from SegmentRecord (for example, if the flags byte has 0x01, it sets FIN bit = 1, 0x02 -> SYN bit = 2, etc.). - Computes payloadOff = off + dataOffset (start of TCP payload) and payloadLen. The payload length is computed as ipPayloadLength - TCP_header_length for IPv4, or for IPv6 similarly using ipPayload. It also ensures not to go beyond captured length. The code clamps negative lengths to 0. - It then populates a new Decoded object with all these fields. The payload is not copied here – only the offset and length are recorded. - The method returns the Decoded instance if successful, or null if the packet isn’t a valid TCP segment with payload info.
Interaction & Usage: TcpDecoder is used in the capture stage (by CaptureMain and also by CaptureLoop in the integrated scenario). It provides a quick filter: only packets that decode successfully as TCP (and usually with payload) are passed on. This prevents non-IP traffic or non-TCP traffic from going further into HTTP assembly logic. The decoder does produce strings for IP addresses (e.g., "192.168.0.1"), which are used in SegmentRecord and later for flow identification.
Performance considerations: - The decoder operates on raw byte arrays very efficiently, using bitwise operations and manual byte extraction. It avoids creating objects except for the final Decoded (and IP address strings). - IPv4 address conversion constructs a string per packet for src and dst. This is some overhead; however, converting to text early makes it convenient to use as keys later (for flows) and to log. If performance became an issue, one could keep binary addresses and convert later, but the simplicity here is likely sufficient. - The code handles VLAN tagged packets, which is important for capturing in environments with VLANs – it will correctly skip the VLAN header. - It ensures no out-of-bound reads by length checks at each step, making it robust against short packets. - The heavy lifting of actual packet capturing is done in native code (libpcap). TcpDecoder is just parsing a copy of those bytes. Given typical packet sizes (a few hundred bytes for HTTP segments), this parsing is quite fast in Java, especially since it avoids allocations in the inner loop except for strings. - One micro-optimization visible: it uses Byte.toUnsignedInt and bit shifting to assemble multi-byte values, rather than higher-level ByteBuffer wrappers, to avoid unnecessary object creation.
Extensibility: - If support for additional protocols is needed (for example, UDP or ICMP), one could extend this class or create similar decoder utilities. For UDP, a similar approach would parse ports and length. - If capturing on non-Ethernet links (like loopback or others), the decoder would need to be adapted (currently it assumes Ethernet header). - The Decoded class could carry more info if needed (e.g., acknowledgment number, window size) but for HTTP capture that isn’t necessary. - Right now it returns null for non-TCP; if we wanted to pass through UDP packets (perhaps to log DNS or other info), we could modify it to return a different type or include a protocol field.
External Libraries: None. This class uses only core Java. All decoding logic is manual, without reliance on java.nio or external parsing libraries, to maximize control and minimize overhead.
AssembleMain.java
Summary: AssembleMain is the second stage in the sniffer pipeline. It reads the raw segment files produced by the capture stage (.segbin files) and reconstructs HTTP conversations. This program iterates through captured segments in order, feeds them to an HttpAssembler, and writes out assembled HTTP messages to an output directory. In essence, it transforms a sequence of TCP segments into a stream of HTTP requests and responses.
Detailed Operation: - Inputs/Outputs: It expects an input directory containing segment files (via --in, default ./cap-out) and an output directory for HTTP results (--out, default ./http-out). It creates the output directory if it doesn’t exist. - Initialization: - Logs the start, noting from which directory it is reading and where it will write HTTP data. - Sets up an HttpStreamSink for output. In this case, it uses new SegmentSink(...) with a configuration to rotate files hourly or by size (e.g., hourlyGiB). The SegmentSink will handle writing assembled HTTP messages as large contiguous blobs plus an index (more on this in SegmentSink analysis below). - Creates a SessionIdExtractor with empty lists for cookies and headers (meaning session extraction is effectively disabled initially; it’s a placeholder for future enhancement). - Constructs an HttpAssembler asm = new HttpAssembler(sink, sessionExtractor). This is the core assembler that will accept TCP segments and produce HTTP message events to the sink. - Creates an instance of DirBook (a private inner class in AssembleMain). DirBook is a heuristic tracker to determine the direction of each TCP flow (client-to-server vs server-to-client). It records the first-seen source of a flow as the “client” and uses that consistently. It provides a method fromClient(SegmentRecord r) which returns true if the segment is traveling from the client side or false if from server side, based on the stored mapping for that 4-tuple. This is necessary because the capture doesn’t label segments as request or response; the assembler needs to know which side is which. - Processing Loop: - It opens a SegmentIO.Reader on the input directory. This reader concatenates all .segbin files (sorted by filename, which in this design correlates roughly with capture time) and allows sequential iteration over all captured segments in chronological order. - It then enters a loop reading SegmentRecord objects one by one via reader.next(). For each segment: - It uses DirBook.fromClient(record) to determine direction (fromClient will return true for segments from the client->server, and false for server->client). - It calls asm.onTcpSegment(...) on the HttpAssembler, passing: - The timestamp (microsecond precision), - Source IP, source port, dest IP, dest port, - Sequence number, - A boolean for direction (the fromClient result), - The payload byte array and its length (if payload is null, it passes an empty byte array and length 0; the code defensively ensures a non-null array), - A boolean fin indicating if the segment had the FIN flag (end of TCP stream). - The assembler will internally buffer and parse these segments. When a full HTTP message (request or response) is assembled, it will invoke the configured HttpStreamSink (SegmentSink) to write out the message. - A counter total is incremented for each segment processed. Every 65,536 (0x10000) segments, it logs a progress message (“Processed X segments”). - The loop continues until reader.next() returns null, meaning no more segments (i.e., all files exhausted). - After the loop, it logs a “Done” message with the total count of segments processed.
Architecture & Design Notes: - AssembleMain is designed to run after capture. It decouples assembly from capturing – this is useful because assembly (HTTP parsing, writing to disk) might be CPU-intensive and can run at its own pace, possibly slightly behind real time, or even on a different machine. - The use of SegmentIO.Reader abstracts away file handling; no matter how many segment files were created by capture, AssembleMain just iterates a single stream of SegmentRecord objects. - By feeding segments in sorted time order, it largely preserves the original packet order as seen on the wire. The assembler requires in-order segments per flow to function correctly. Since capture wrote them in near-original order, this should hold, except possibly segments that were captured out-of-order between files during rotation (though the capture rotation was designed to avoid splitting a flow mid-message). - DirBook is a simple concurrency-safe (using ConcurrentHashMap) way to label flows. It’s crucial for HTTP parsing: knowing which side is the client tells the assembler whether bytes should be appended to a request or a response buffer.
Performance Considerations: - The assembly process is single-threaded in this design. It processes one segment at a time in the order they were captured. This ensures that within each TCP connection, data is fed serially to HttpAssembler (which internally handles out-of-order segments if any). - HttpAssembler itself is optimized for performance (discussed separately), using fixed-size buffers and minimal locking. Feeding it sequentially here aligns with its design. - Writing output: SegmentSink operates with an internal queue and a worker thread for file I/O. This means AssembleMain’s main thread is mostly doing CPU work (HTTP parsing) and delegates disk writes to another thread, which is good for throughput. The logging every 65k segments is minor overhead. - The total memory used depends on HttpAssembler buffering and any backlog in SegmentSink’s queue. The config used (hourlyGiB) implies the output will be chunked into hourly files ~1 GiB each for manageability. - If the input segment files are very large (e.g., representing hours of traffic), this process can also be long-running. It doesn’t inherently parallelize assembly across multiple cores (one might imagine parallelizing by flow, but that’s a complex change). As is, it will utilize one core for assembly and one for writing. In typical usage, that can be sufficient (since capture is I/O-limited and assembly can often keep up, but if not, assembly will just lag behind). - Reading from disk is sequential and uses buffered I/O (see SegmentIO.Reader with 1 MiB buffering). This should be efficient on modern drives. The program does a single pass over the data.
Extensibility: - Session ID extraction: The SessionIdExtractor is included but currently given no patterns (empty lists). In the future, a developer can provide cookie names or header regexes to track session identifiers. The HttpAssembler will call this to tag messages with session IDs (so that request/response pairs or multiple requests from the same user session can be correlated). - Output sink customization: The code currently fixes HttpStreamSink to SegmentSink (which writes to file). If one wanted to send assembled HTTP messages elsewhere (e.g., a database or over network), they could implement HttpStreamSink differently and use that. The assembler doesn’t know or care what the sink does, as long as it meets the interface. - Filtering or transformation: At this stage, one could introduce filtering (for example, only assemble certain flows or hosts). Right now, it processes everything. A developer could check the segment’s addresses or ports and decide to skip calling asm.onTcpSegment for unwanted traffic, or similarly extend HttpAssembler to filter at the HTTP level (e.g., ignore images or certain content types). - Parallel processing: While not trivial, one could attempt to parallelize by splitting the input by flows (for example, by hashing flows and processing different sets of flows in different threads). The current design does not do this due to ordering complexity, but it’s an area a future maintainer might explore if needed for performance on multi-core systems.
External Libraries: None beyond core Java. AssembleMain uses Java file I/O and concurrency (ConcurrentHashMap). All heavy parsing logic is handled by HttpAssembler (internal logic) and writing by SegmentSink. There are no external dependencies in this stage (no libpcap here, since input is from files).
CaptureLoop.java
Summary: CaptureLoop provides an alternative approach to capturing and processing traffic in real-time. It ties together live packet capture, HTTP assembly, and output, all in one continuous loop. Essentially, it integrates what CaptureMain and AssembleMain do into a single process. This was likely used for an interactive or legacy mode, where captured packets are immediately assembled into HTTP messages and possibly printed or logged.
Key Elements: - Fields: - A Pcap instance (pcap) for capturing packets. - A PacketSink interface instance (sink) for output callbacks. - A boolean showHeaders indicating if full headers should be shown in output (or just the first line). - Internally, it also sets up an HttpAssembler httpAsm and a SegmentSink segmentSink for writing assembled messages. It creates a SessionIdExtractor too (possibly with some default or config). - It maintains a concurrent map of flows (ConcurrentMap<FlowKey, FlowDir> flowDirs) – this suggests it tracks flows similar to HttpAssembler, possibly for out-of-order management or to know which flow a packet belongs to. - PacketSink Interface: An inner interface with methods onHttpLine(long tsMicros, String text) and onUdp(long tsMicros, String src, int sport, String dst, int dport). This is a callback mechanism for the loop to output information: - onHttpLine is intended to output an HTTP request/response line (with timestamp and maybe some contextual info). - onUdp might be used to log UDP packets (the capture loop can optionally handle UDP, perhaps to note their presence). - Constructor: Takes a Pcap (capture provider), a PacketSink, and a showHeaders flag. It initializes the assembler and sinks: - It creates a SegmentSink with default rotating file config (likely writing to some default path like "out" or current directory). - It creates an HttpAssembler with that segmentSink and a new SessionIdExtractor. - These will be used to assemble HTTP messages and write them out asynchronously, in addition to immediate logging via PacketSink. - FlowKey/FlowDir: Although not fully visible in our snippet, given the context: - FlowKey is likely a small class or static nested class representing a TCP flow (e.g., containing two IPs and ports). This could be similar to what HttpAssembler uses internally to key flows in a map. - FlowDir likely holds per-flow state (maybe similar to HttpAssembler.Conn concept, with buffers etc). It might duplicate some of the assembler functionality, or it might be that CaptureLoop was an earlier implementation of assembling logic, before HttpAssembler was written. There could be some overlap in purpose. - Method runForever(String iface, int bufMb, int snap, int timeoutMs, String bpf): This method starts the capture: - It uses the provided Pcap object (pcap) to open a live capture: pcap.openLive(iface, snap, true, timeoutMs, bufBytes, true) – similar to what CaptureMain does (promiscuous mode true, immediate mode true). It uses a try-with-resources for the returned PcapHandle (h) and also for segmentSink (so that both will be closed on exit). - If a BPF filter is provided, it calls h.setFilter(bpf). - Then it enters a loop: while (h.next((tsMicros, data, caplen) -> { ... })) { }. This is capturing packets one by one via the callback lambda. - Inside the callback: - It likely mirrors CaptureMain logic: decodes the packet (perhaps using the static methods in Decoders or similar to identify IPv4 vs IPv6, or directly using TcpDecoder). - If the packet is TCP and decodable, it will feed it to the HttpAssembler (httpAsm). Specifically, it might call something analogous to httpAsm.onTcpSegment(...). Since HttpAssembler already manages flows and HTTP parsing, leveraging it would be logical. - However, the presence of PacketSink.onHttpLine suggests that CaptureLoop might also directly extract some info to print as soon as the HTTP start line is seen. Indeed, the snippet we saw references HttpPrinter.firstLineAndMaybeHeaders(...) and then calls sink.onHttpLine(ts, "... %s:%d→%s:%d %s".formatted(...)). This implies: - It prints a log line for each HTTP message detected (with timestamp, source->dest arrow, and the start line or headers). HttpPrinter is used to format the first line (and headers if showHeaders is true) from the raw packet bytes. - This likely occurs when a packet contains the beginning of an HTTP message. Possibly, whenever a TCP segment with the PSH flag or any payload arrives on port 80/HTTP, they treat it as potential HTTP. They may not fully parse multiple segments – this is more of a “best-effort” printer. (It might print each segment’s payload interpreted as text, which could cut off between packets of a large message.) - The design is a bit unclear: Using HttpAssembler concurrently with manually printing might duplicate effort. Perhaps CaptureLoop was an earlier approach where they tried to parse HTTP lines directly from packets. The flowDirs map might have been for reassembling headers across multiple segments. - It’s possible that CaptureLoop predates the more robust HttpAssembler, and it attempted its own simple reassembly: e.g., FlowDir might contain a buffer for assembling header lines until \r\n\r\n is seen, then it would call sink.onHttpLine once a full header is ready. - Meanwhile, it uses segmentSink (the one backing HttpAssembler) to also write out the data to files, ensuring nothing is lost. - The loop also handles non-TCP: - If a packet is UDP (e.g., DNS or other), it may call sink.onUdp(ts, src, sport, dst, dport). There likely is a check using methods from Decoders (which can detect IPv4 and maybe identify UDP) to identify UDP packets to log them. - The loop runs indefinitely until the process is terminated (by user interrupt or exception).
Architecture Considerations: CaptureLoop essentially fuses capture and assembly: - It uses the same HttpAssembler and SegmentSink that the two-step approach uses, but now in the same thread as capture. - It provides immediate feedback through the PacketSink for each HTTP message line. This is useful for monitoring traffic in real-time (like a live “tail” of HTTP requests). - It also writes out the full data to SegmentSink (which presumably writes the NDJSON and blobs just as in the two-step process, but now immediately). - One challenge in this design is performance: doing capture and assembly in one thread means the throughput might be lower. It’s essentially doing all the work synchronously that the two-stage pipeline would do asynchronously. For high traffic, this could lead to packet drops if assembly or I/O can’t keep up with capture.
Performance Considerations: - Because capture and assembly share the same thread here, the time to process each packet (including possibly writing to disk and printing to console) adds to the interval before the next packet is handled. If packets arrive faster than this processing, the pcap buffer could overflow. The design likely relies on the fact that writing to SegmentSink is offloaded to its own thread – indeed, SegmentSink has an internal queue and worker, so disk I/O is asynchronous even in this loop. That helps a lot. The main thread only enqueues the data to be written. - Printing to console (onHttpLine) is synchronous and comparatively slow (I/O + string formatting). If verbose headers are enabled, printing whole headers is even slower. This mode is thus intended for debugging or low-volume scenarios. In high-volume, you’d likely keep showHeaders=false to print just the request line, or disable printing entirely (maybe by not using StdoutSink). - Memory: Similar to other stages, it uses buffers in assembler and queue in SegmentSink. It also might accumulate some state in flowDirs for partially seen headers. But all flows state is in memory here (like HttpAssembler with up to MAX_CONNS flows). That could grow if many flows are ongoing. - The advantage is zero inter-process delay – assembly happens immediately, so you don’t need to wait or coordinate between capture and assemble processes.
Extensibility: - This class could be extended to support different PacketSink implementations. For example, one could implement a sink that writes to a GUI, or filters certain messages. The provided StdoutSink is one implementation (printing to System.out). - If more complex real-time analysis is desired, developers can implement the PacketSink methods accordingly (e.g., push events into a monitoring system). - The showHeaders flag is a simple mechanism to toggle how much is printed; more sophistication (like print only certain headers or mask sensitive data) could be added. - The flowDirs and internal reassembly logic, if present, could be refined or removed in favor of relying entirely on HttpAssembler. If HttpAssembler is fully used, CaptureLoop might not need to manage flows itself. Future maintenance might simplify this if duplication exists. - In its current form, CaptureLoop might not be actively used in the newer pipeline (which prefers separate processes). But it serves as a good example or tool for testing. A future developer might either remove it if redundant or enhance it to be a more user-friendly “live capture” tool (perhaps adding interactive features or better filtering for live mode).
External Libraries: Like other capture code, it uses the Pcap interface (so JNR libpcap underneath) for packet capture. Otherwise, it’s core Java (concurrency utils, etc.). The assembly and sink are internal classes (no external library for HTTP parsing – it uses our HttpAssembler). So the main outside dependency here is still libpcap via JNR.
Decoders.java
Summary: Decoders is a small utility class with static methods to assist in low-level packet inspection. It primarily focuses on distinguishing IPv4 packets and handling IPv4 header details. It’s essentially a helper for the capture loop or decoder logic, separate from the main TcpDecoder.
Provided Methods: - static boolean isIpv4(byte[] p, int caplen): Checks if a captured frame appears to be an IPv4 packet (on Ethernet). Specifically, it returns true if caplen >= 14 and the EtherType field at bytes 12–13 of the frame equals 0x0800 (which signifies IPv4 in Ethernet II framing). This is a quick check used to ignore non-IPv4 traffic. - static int ipv4HeaderLen(byte[] p, int ipOff): Calculates the length of an IPv4 header. It takes the offset in the packet where the IP header starts (for Ethernet, typically 14). It reads the first byte at that offset, which contains the version and IHL (Internet Header Length). If the version is not 4, it returns -1 (meaning not an IPv4 header at that location). Otherwise, it takes the lower 4 bits (IHL) and multiplies by 4 to get the header length in bytes. For example, if the byte is 0x45, that means version 4 and IHL=5 (5 * 4 = 20 bytes header). The method returns the header length, or -1 if the data is not a valid IPv4 header.
Usage: These methods would typically be used in a packet processing loop to quickly filter and parse packets: - isIpv4 could be used to decide whether to even attempt further IPv4/TCP decoding on a frame. - ipv4HeaderLen could help in parsing an IPv4 packet when you need to skip over the IP header (for example, to find the start of the TCP header). In our codebase, the TcpDecoder actually performs this logic internally without calling Decoders. Possibly CaptureLoop might have used these methods in its own implementation of parsing (if any). - The class is public final with a private constructor, meaning it’s just a static method container. The methods use static imports from sniffer.common.Bytes (like u8() and u16be() functions) to easily extract bytes.
Architecture: Decoders segregates some low-level protocol knowledge (EtherType constants, IP header layout) from the main decoder. It’s very minimal. It does not cover IPv6 at all – presumably because IPv6 decoding was handled explicitly in TcpDecoder instead. This could be an indicator that Decoders was written for an earlier simpler use case (maybe initial version only handled IPv4) or for other parts of code that needed just these checks.
Performance: Both methods are just a couple of arithmetic operations and array accesses: - isIpv4 does two byte accesses and comparisons. - ipv4HeaderLen does one byte access and some bit ops. These are negligible in cost, typically inlined by the JIT. They are safe operations as long as the offsets are within bounds (the methods assume the caller ensures ipOff is within array length, etc.). They return quickly without allocations.
Extensibility: - If needed, similar methods could be added (e.g., isIpv6, or methods to parse UDP headers, etc.). In fact, having an isIpv6 might be logical for symmetry, but the codebase handled IPv6 differently (within TcpDecoder). - The existence of this class suggests it could be expanded as a central place for various packet decode helpers. Right now it’s very limited. A future developer might fold this logic into TcpDecoder for simplicity, or extend it if new decoding routines are needed elsewhere. - The methods assume Ethernet as the link layer. If we needed to support other link types (like raw IP capture, loopback headers, etc.), additional logic would be needed, but likely this class wouldn’t be directly used in that case.
Dependencies: It uses sniffer.common.Bytes for byte manipulation (that itself is a small utility for unsigned bytes). No external libraries. It’s pure Java logic dealing with byte arrays.
HttpAssembler.java
Summary: HttpAssembler is the heart of the system’s HTTP reassembly logic. It consumes a stream of TCP segments (in order per flow) and extracts complete HTTP/1.x messages from them, handling out-of-order packets, TCP stream reassembly, and HTTP parsing. It is designed for high throughput and low garbage generation, using fixed buffers and caching techniques. When it reconstructs an HTTP request or response, it streams the output through an HttpStreamSink (for writing to disk or elsewhere) without buffering entire messages in memory. This allows handling large bodies efficiently.
Internal Architecture: HttpAssembler uses several inner classes and data structures: - Connection Tracking: It identifies flows by a tuple of client IP, client port, server IP, server port. Internally it likely normalizes this so that each connection is represented in one direction (e.g., always stored as client->server key). We see references to a flows map (probably a ConcurrentHashMap<FlowKey, Conn>). Each Conn (connection) object holds state for that TCP connection. - Conn (Connection state class): The code snippet shows class Conn { final Dir c2s = new Dir(); final Dir s2c = new Dir(); final Deque<String> pending = new ArrayDeque<>(); ... }. This suggests: - A Conn contains two Dir objects, one for client-to-server direction, one for server-to-client. Each Dir likely buffers data until a message is complete in that direction. - pending is a deque of transaction IDs for requests that have been seen but whose responses haven’t yet been fully processed. This is used to match responses to requests. For example, when a request is completed, its ID (a ULID string generated) is pushed onto pending. When the corresponding response is later assembled, the assembler will pop the matching ID (for HTTP/1.x, responses come in order). - lastSession might store the last seen session ID (if SessionIdExtractor found one in a request or response) to possibly tag the opposite message if needed. - lastSeenNs tracks the last time a segment was seen for this connection (for timeouts/eviction). - Dir (Directional state class): From snippet: class Dir { TreeMap<Long, byte[]> ooo = new TreeMap<>(); int oooBytes = 0; long nextSeq = -1L; /* plus buffers... */ }. - Each Dir handles reassembly for one direction (either request or response stream). - nextSeq is the next sequence number expected in this direction’s stream. Initially -1, probably set when a SYN is seen or when first segment arrives. - ooo (out-of-order map) stores any received segments that came out-of-order (sequence number not equal to next expected). It maps sequence numbers to their payload bytes. This allows buffering a limited amount of out-of-order data until the missing pieces arrive. - oooBytes tracks total bytes in the ooo map for that direction (used to enforce a memory limit for out-of-order data). - Fixed Buffers: The comment suggests fixed-size buffers for headers and body. Likely each Dir has byte arrays for assembling the current message’s header and body: - e.g., a byte[] hdrBuf and int hdrLen to accumulate header bytes until the end-of-headers (\r\n\r\n) is found. - a byte[] bodyBuf and associated counters to accumulate the message body if needed (for instance, if chunked encoding is present, or content-length and not streaming out immediately). - They emphasize no dynamic growth – these buffers are allocated up front (sizes configurable via system properties like sniffer.maxHeaderBytes default 32KB, sniffer.bodyBufCap default 32KB). This means headers longer than 32KB or an in-memory body exceeding 32KB would trigger some backpressure or special handling (like chunked bodies are largely streamed to sink without fully storing in memory). - As segments arrive in order, the Dir will copy data into the appropriate buffer: - If currently expecting headers (not yet saw the end-of-headers marker), bytes go into hdrBuf. - Once headers are complete, it parses them to determine message length or transfer encoding, etc. Then it knows how to handle the body: - If no body (e.g., GET request with no payload, or response with no content and no chunked encoding), it can finalize the message immediately. - If content-length is known or chunked encoding, it will stream the body: * Possibly it does not buffer the whole body in memory. Instead, it uses the HttpStreamSink to write out body bytes as they come (for “streaming body writes”). But to do that, it might still stage some bytes in a small buffer before pushing. * If chunked, it might need to accumulate chunk data until complete chunk header lines are read; but more likely, it treats chunked as just another case to stream out incrementally and leaves actual de-chunking to the final stage (the Poster). - The fixed bodyBuf might be used as a staging area for cases where the sink cannot keep up or for a small portion of body between yields, but ideally data is passed through directly. - Negative Flow Cache: The static fields in the class (NEG_CACHE_LOG2, etc.) indicate a structure (likely an array or bitset of size 2^16 by default) to remember flows that are not HTTP. For example, if a segment arrives on some port that looks nothing like HTTP, the assembler can mark that flow as non-HTTP so it will quickly ignore future segments from it (saving parsing overhead). The negKey(src, sport, dst, dport) likely computes a hash or index for the flow, and negCached(nkey) checks if we have recently flagged it. If yes, return immediately (skip processing). - NEG_TTL_NS (10 seconds default) suggests an entry in negative cache expires after 10s – after that, maybe the assembler will give that flow another chance in case it later starts HTTP (unlikely, but defensive). - This prevents repeatedly trying to parse non-HTTP traffic (like other TCP services that might be mirrored in capture if filter wasn’t strict). - Flow Cache (Hot Path optimization): The snippet references last0 and last1 caches. This sounds like an LRU of size 2 for the most recent flows: - Possibly last0 and last1 are small structs holding a FlowKey and corresponding Conn. Every time a segment is processed, if it matches the last seen flow, it hits last0 (fast path, no map lookup). If it’s a different flow, they check last1, etc. This avoids constant hash map lookups for back-to-back packets of the same connection, which is a common case (packets often arrive in bursts per connection). - If a new flow comes, they update these mini-cache entries accordingly (as seen by logic: check last0, then last1, else map lookup, then possibly swap them). - Transaction ID generation: Each new HTTP transaction (request/response pair) gets a unique ID. The code uses HttpIds.ulid() to generate an ID string (26-char Base32 ULID). The logic in the assembler: - When a request message is identified as complete, it generates a tid via HttpIds.ulid() and assigns it to that request. It then pushes that ID into the pending deque for the connection. - When a response message is identified, instead of generating a new ID, it checks if there is a pending ID (meaning there’s a request waiting for a response) – if so, it uses the same tid for the response and pops it from the deque (ensuring request and response share an ID). - If, for some reason, a response comes with no matching request (shouldn’t happen in well-formed HTTP, except perhaps if we started capturing mid-stream), they might generate a new ID or handle it as an orphan. - Streaming Output: HttpAssembler doesn’t directly write to files; instead it calls methods on the provided HttpStreamSink: - streamSink.begin(meta, headersBytes) – when it has finished reading the headers of a message, it calls begin. It passes a Meta object describing the message (which includes the id, type "REQ"/"RSP", session id if any, src/dst addresses, and timestamp of first byte) and the raw bytes of headers. The sink will record these (write headers to blob and note offsets). - As body data arrives (possibly in subsequent segments or already partly in the same segment), it calls streamSink.append(handle, bytes, offset, length) for each chunk of body. The handle is a token returned by begin (e.g., a StreamHandle representing that open message output stream). - When the message is complete (either because the specified Content-Length worth of bytes are sent, or a terminating chunk, or a FIN for messages with no length), it calls streamSink.end(handle, tsLast) with the timestamp of the last byte. The sink will then finalize the record (e.g., write an index entry with total body length and close out the handle). - This streaming approach allows the assembler to never buffer the entire body in memory; it pushes data out as it comes. It’s a key design for performance with large payloads.
HTTP Parsing details: - The assembler must detect the end of headers (\r\n\r\n). It likely does this by examining the hdrBuf content as it’s filled. Once found, it parses the headers: - At minimum, it will parse out: - The start line (Request line or Status line) – possibly to store as Meta.firstLine or to log/print if needed. - The Content-Length header (if present) or Transfer-Encoding (to detect chunked). - Possibly the Connection header (to know if the connection will close – though this might not be needed, as FIN will signal end anyway). - It passes all headers raw (they will be written out as captured). It might not decode them beyond what’s needed. - It uses the SessionIdExtractor: for requests, call fromRequestHeaders (passing a map of header names to values) to get a session ID if any; for responses, call fromSetCookie on any Set-Cookie headers. The returned session ID (if not null) is stored in the Meta for that message. - It determines how to handle the body: - If Content-Length: N is present (and not chunked), it knows it needs exactly N bytes of body. It can then track how many bytes have been appended so far and know when to end. - If Transfer-Encoding: chunked, it knows the body is chunked. Interestingly, the assembler does not itself decode chunked encoding; it will likely treat the chunked data as just the body and stream it out. The actual de-chunking is deferred to the PosterMain stage (where they have logic to decode chunked if requested). So the assembler’s job is just to capture all bytes until the end of chunked stream (which is indicated by a terminating “0\r\n\r\n” chunk). - If neither header is present (HTTP/1.0 or certain responses), it may use the presence of FIN or connection close to decide message end (this is tricky but typical: if no length and not chunked and it’s a response to a request that isn’t Keep-Alive, then end of connection = end of message). - If it’s a response to a HEAD request, the headers might indicate content-length but no body will actually be sent; assembler should handle that properly (likely by using request method knowledge if it had that – not sure if it parses the method). - It might have limited knowledge of status codes (e.g., 204 No Content or 304 Not Modified have no body despite possible Content-Length header, etc.). It’s unclear if assembler handles those specifically or just relies on the fact that no bytes will come. - As data segments arrive: - If they contain bytes that belong to the current message’s body and the message is not yet complete, the assembler will directly call append to the sink with those bytes. - It may buffer a small amount if, say, the segment ends in the middle of a chunk header in chunked encoding. But given the complexity, likely it treats chunk headers as part of body and just streams everything to output, letting the final stage handle it. - Once a message (request or response) is complete: - It finishes the output via end(handle, tsLast). - If it’s a request, it pushes the transaction ID to pending (and potentially logs it if verbose). - If it’s a response, it pairs with the corresponding request ID (as mentioned) and possibly outputs some combined info if needed (the actual pairing is mainly by ID in logs). - It then resets state for that direction to be ready for the next message on the same connection (HTTP persistent connections). That means: * Clear or reuse the header buffer (set length back to 0). * Reset nextSeq for that direction to what’s expected for next message (which would just continue from current sequence if pipelining, or if not pipelining, the next segment for next message will follow). * Keep any overflow bytes: It’s possible that a segment contained the end of one message and the beginning of the next (in pipelined requests or when response follows immediately). The assembler likely handles this by noticing extra bytes after the end-of-message and then immediately processing them as the start of a new message within the same Dir. This might be done by looping within the segment processing. - Connection closing and eviction: - If a FIN is encountered, it’s passed in onTcpSegment(..., fin=true). The assembler will mark that side as closed. If both sides closed (FIN from both), then the connection is done. It may remove the Conn from the map immediately or after a timeout. - Even without FIN, they have an idle timeout (IDLE_NS = 30s by default). They also have EVICT_EVERY = 16384 (meaning every 16384 segments processed, they might scan for idle connections to evict). This prevents the flows map from growing indefinitely with dead connections. - They cap total connections at MAX_CONNS (20,000 by default). If that is exceeded, new flows might be dropped or not processed to avoid memory bloat. - They cap out-of-order buffer memory globally with MAX_OOO_GLOBAL (64 MB by default). The static OOO_INUSE (AtomicLong) tracks total bytes in all ooo buffers. If a new out-of-order segment would exceed this, they likely drop it or apply backpressure (possibly by pausing reading – though since this is offline assembly, backpressure might just be a sleep via LockSupport.parkNanos(BACKPRESSURE_NS) if needed to slow down).
Performance Highlights: The class documentation explicitly notes its focus on performance: - It avoids allocating new buffers per message – reuses fixed buffers. - It uses small caches (last0, last1) to reduce map lookups for hot flows. - It uses a negative cache to avoid re-parsing non-HTTP flows repeatedly. - Bounded buffering ensures memory use is controlled even under heavy load (it won’t allocate more memory if, say, a huge burst of out-of-order data comes – it will either drop or throttle). - It likely uses lock-free structures where possible; the flow map is a concurrent map, but in the assembly single-threaded scenario, contention is low (if assembly is single-thread, it could even be a regular HashMap; but if integrated, maybe multiple threads could call it – though onTcpSegment isn’t explicitly thread-safe). - The design expects segments to be already in order per flow for normal operation (since CaptureMain writes in order and AssembleMain reads sequentially). Out-of-order handling is mainly for minor reordering or capturing mid-stream. So in typical cases, the ooo TreeMap is often empty or small, which means minimal overhead. - The heavy string operations (like building header maps or parsing ULIDs) are relatively infrequent (per message, not per segment). And even those are done without using external regex or libraries (except for SessionIdExtractor patterns), to keep throughput high.
Extensibility: - This class is quite complex and specific to HTTP/1.x. Extending it to HTTP/2 would be a major project (HTTP/2 is binary and connection multiplexed, requiring a completely different approach). Likely, if HTTP/2 is needed, one would create a separate assembler. - If wanting to handle other protocols (like WebSockets upgrade, or TLS decryption), that would involve hooking in at appropriate points: e.g., for TLS, decrypting would have to be done before HTTP parsing – not trivial and beyond the scope of this code. - The assembler could be extended to output additional metadata if needed (for example, calculating latency between request and response – it has timestamps which could be used). - For now, it focuses on correctness and performance for HTTP/1.1. As an extender, one should be cautious altering things here due to the delicate performance balancing. For instance, increasing buffer sizes might let larger headers through but also increase memory usage per connection. - One could adjust the static tunables via system properties (mentioned constants are overridable via -D). That’s already an extensibility mechanism for tuning performance without code changes. - The SessionIdExtractor integration means one can plug in new patterns or cookie names externally (no code change) to start capturing session info in the output.
External Libraries: None. All parsing is custom. It uses core Java collections and concurrency utilities (like ConcurrentHashMap, AtomicLong). It doesn’t use something like Netty or HTTP libraries – it’s all purpose-built. This is deliberate to avoid overhead and to fine-tune for this use case.
HttpIds.java
Summary: HttpIds is a utility class for generating unique identifiers for HTTP transactions. It implements ULID generation (Universally Unique Lexicographically Sortable Identifier), producing 26-character strings that encode a timestamp and randomness. These IDs are used as transaction IDs (tid) to tag each HTTP request/response pair.
How ULID generation works in HttpIds: - The ULID format is 128 bits: 48 bits timestamp (in milliseconds) + 80 bits of randomness, encoded in Crockford’s base32 (which omits ambiguous characters). - HttpIds defines a char array ENC of length 32 containing the alphabet "0123456789ABCDEFGHJKMNPQRSTVWXYZ" (note the omission of I, L, O, U for clarity). This is the encoding alphabet. - The main method static String ulid(): - It takes the current time in milliseconds (Instant.now().toEpochMilli() for timestamp). - It obtains two random 64-bit values (r1 and r2) from ThreadLocalRandom.current().nextLong(). These provide 128 bits of randomness, though it will actually use only 80 bits. - It then encodes the timestamp and parts of the randomness into a 26-character array: * Likely, it calls an internal helper enc48(time, out, 0) to encode 48 bits of timestamp into the first 10 characters (since 32^10 is 2^50, enough for 48 bits). * Then encodes the 80-bit random value into the remaining 16 characters. Possibly by splitting r1 and r2 appropriately. - The function returns the resulting string. - The code mentions functions like enc48 (we see in the snippet a call enc48(time, out, 0)). There might also be enc40 or similar for the random part. They likely shift and mask bits, mapping each 5-bit chunk to a character in ENC. - The result is a ULID string such as "01ARZ3NDEKTSV4RRFFQ69G5FAV" (example format).
Usage: Every time a new HTTP request is encountered by HttpAssembler, it calls HttpIds.ulid() to assign a unique ID. This ID is then used to link the request with its corresponding response, and is written in the output (NDJSON index and eventually the file names or content of .http files). The IDs are sortable by time (since they start with time-based component), which can be useful if you sort transactions by ID.
Performance Considerations: - ULID generation is relatively efficient: - It uses ThreadLocalRandom, which is fast and thread-safe without locks for each call. - It does some bit shifting and array accesses to map bits to characters. This is much cheaper than, say, using UUID and then base32 encoding it via big libraries. - It produces a fresh String of 26 chars for each call. The allocation of 26-char arrays and constructing a String from it is minor overhead in the context of processing an HTTP message with potentially much larger content. - Because part of the ULID is time-based, if many IDs are generated in the same millisecond, the time part is the same but the random part will differ, which is fine. The randomness covers collisions sufficiently (2^80 possibilities each millisecond). - The generation is lexicographically sortable by time, meaning if you compare two ULIDs as strings, their order reflects creation order (as long as the generation didn’t exceed the same millisecond with colliding randomness, which is astronomically unlikely). This is useful if the output needs to be time-sorted lexicographically (like in logs or filenames).
Extensibility: - HttpIds currently only provides ULID generation. If needed, it could implement other ID formats or parsing functions (none are present now, and likely not needed). - The ULID approach is well-chosen for this scenario, but if a different ID scheme was desired (e.g., shorter IDs, or numeric IDs), one could replace this utility and the assembler’s usage. However, ULIDs have the advantage of being globally unique and time-sortable, which fits distributed capture scenarios too. - It doesn’t take any configurable input (like no namespace or prefix). It’s simple and stateless (aside from time). - It may be worth noting that ULIDs here use the system time in milliseconds. If system clock is not monotonic or jumps, ULIDs could reflect that (e.g., if clock goes backward, ULIDs might sort incorrectly). In most cases this isn’t an issue, but it’s a known consideration with time-based IDs. Given this is for logging/analysis, not a critical problem.
External Libraries: None. This is custom code – it doesn’t use any UUID library or similar. It relies on Instant and ThreadLocalRandom from the JDK.
HttpMessage.java
Summary: HttpMessage is a sealed interface that defines the structure of an HTTP message in this system. It has two implementations, HttpRequest and HttpResponse, which hold the details of HTTP requests and responses respectively. These classes serve as data carriers for complete HTTP messages (headers + body), primarily in contexts where the system might need to handle a whole message at once (for example, when using the legacy HttpEventSink interface or writing individual files per message).
Interface HttpMessage: - It defines getters for common properties of any HTTP message: - id() – the unique transaction ID (the ULID string). - sessionId() – the session identifier if one was extracted (or null/empty if none). - tsFirstMicros() – timestamp of the first byte of the message (microsecond precision). - tsLastMicros() – timestamp of the last byte of the message. - srcIp()/srcPort() – source IP address and port. - dstIp()/dstPort() – destination IP and port. - rawStartLineAndHeaders() – the raw bytes of the HTTP start line and headers (this is the exact bytes as they were on the wire, likely including the terminating CRLFCRLF). - rawBody() – the raw bytes of the message body. - It also has a default method kind() that returns a String indicating message type, e.g. "REQ" or "RSP". The default implementation is (this instanceof HttpRequest) ? "REQ" : "RSP".
Because HttpMessage is sealed, only HttpRequest and HttpResponse can implement it (as declared by permits clause).
Class HttpRequest: - Declared as final class HttpRequest implements HttpMessage. - It contains public final fields for all the properties: - id, sid (session id), src, dst (IPs as Strings), - sp, dp (source port, dest port), - t0, t1 (timestamps first and last), - head, body (byte arrays containing the raw header+start line section and the raw body). - Its constructor likely takes all these fields as parameters and assigns them. There might be convenience or builder methods not shown, but likely straightforward. Once constructed, it’s an immutable data object.
Class HttpResponse: - Similarly, final class HttpResponse implements HttpMessage with fields: - id, sid, src, dst, sp, dp, t0, t1, head, body (almost the same set). - The difference is just semantic – HttpResponse represents the server’s reply. It might not have “method, path” like a request does, but those are embedded in the head bytes anyway.
Usage in the System: - In the current streaming design, these classes are not heavily used. The HttpAssembler avoids constructing HttpRequest/HttpResponse objects for performance reasons, instead streaming data out. However, they exist for compatibility with the HttpEventSink interface and possibly for debugging or tools. - For example, MessageFileSink might take an HttpMessage object and write it to disk. If using that legacy path, the assembler could create HttpRequest/HttpResponse objects and pass them to an HttpEventSink. In the code, the HttpAssembler(HttpEventSink, SessionIdExtractor) constructor actually throws an exception (indicating it doesn’t support that mode directly). But HttpAssembler(MessageFileSink, ...) uses a trick: it calls the other constructor with a default stream sink, meaning it doesn’t actually produce HttpMessage objects. - So, currently, HttpMessage objects might only be created if someone explicitly uses the SegmentIO.Reader and then manually constructs these (which is not happening in our main flow). Or possibly in unit tests or future extensions. - The HttpPrinter utility might have used HttpMessage in a different design, but it ended up taking raw bytes instead.
Performance Considerations: - These are plain data holders. Constructing an HttpRequest or HttpResponse involves allocating byte arrays for headers and body of that message. That can be expensive if the body is large (it implies reading the entire body into memory). The streaming design specifically avoids this to handle large bodies efficiently. So, these classes would be inefficient for large data (imagine a 100MB response – storing that in a byte array in an HttpResponse object would use a lot of memory). That’s why the system opts not to use this in normal operation. - However, for small messages or low-volume scenarios, using these objects is fine and more convenient to pass around (like encapsulating everything about a message in one object). - They do provide fast access to fields (being public final fields, though typically one would use the interface getters for abstraction). No complex behavior, just data.
Extensibility: - If needed, one could add more methods to the interface (for example, parsed HTTP fields like method, URL, status code, etc.). But that would either require parsing the head bytes or storing those in the objects. Currently, the design doesn’t parse HTTP at this layer – it treats the start line and headers as an opaque byte array. - One might also add convenience methods: e.g., asString() to get a nicely formatted version of the message, or specific getters like method() for requests or statusCode() for responses by parsing the start line. None of those are provided in the code we have, likely to keep it minimal. - The sealed nature means no other implementations can be made outside this file. If a developer wanted a different representation (say an aggregated HttpTransaction class combining request and response), that would be separate from this. HttpMessage is strictly for single messages.
External Libraries: None. This is plain Java. It leverages Java’s sealed interface feature (introduced in Java 15+), which is a modern language feature but doesn’t involve external dependency.
HttpPrinter.java
Summary: HttpPrinter is a utility with static methods to produce human-readable text from raw HTTP message bytes. It is used to extract and format the start line and possibly headers of an HTTP message for logging or display. This is particularly used in the live capture scenario (CaptureLoop) to print incoming HTTP requests/responses to the console in a friendly format.
Primary Function – firstLineAndMaybeHeaders(byte[] p, int start, int end, boolean headers): - This method takes a byte array p (which contains HTTP data), a start index and an end index within that array delimiting the HTTP message bytes of interest, and a boolean headers indicating whether to include the headers or not. - It returns a String that consists of: - The first line of the HTTP message (request line or status line), - If headers == true, possibly the subsequent header lines as well (up to a certain limit), - It will stop at either the end of the headers section or a size cap. - Implementation details: - It sets a limit max on how far to scan: if headers is true, it allows up to 64KB from start (i.e., it will capture headers up to 64 KB); if headers is false, it limits to 4096 bytes (to avoid printing excessively long lines). - It finds the end of the first line by scanning for a newline ('\n') character. It uses a loop incrementing i until newline or until max or end boundary. - It identifies lineEnd as the index of the newline (or max if not found by then). It also computes trimEnd: if the byte before newline is '\r', it backs off one (so it trims the CR from CRLF). - It then constructs a StringBuilder and appends the first line: it creates a new String from p[start ... trimEnd] using standard charset (likely UTF-8). - If headers is true, it then continues from lineEnd: * It advances i past the newline. Then from there until max or end, it appends each byte as a character. Essentially, it copies the subsequent header lines and possibly some of the body if the headers themselves don’t reach max, up to the limit. * It does not appear to explicitly stop at the end of headers (double CRLF) – given the name, one might expect it to stop at the blank line after headers. The implementation we guess simply prints everything up to max bytes. However, since in usage headers parameter likely means “include headers”, they probably intend to print headers until the blank line. Possibly the code does check for double newline sequence and stops there. The snippet doesn’t show that logic clearly, but a prudent implementation would break out if it encounters \r\n\r\n (end of headers). * It’s possible they rely on the capture loop passing end as the end of headers already. But in CaptureLoop, they call HttpPrinter.firstLineAndMaybeHeaders(pkt, pay, caplen, showHeaders), where pay is likely the offset of payload in the packet and caplen is length of data. If showHeaders is true, they might be passing the entire packet from start of HTTP data to the end of captured data. That could include part of the body if it was in the same packet. However, printing the body isn’t desired, so probably not. * Perhaps in CaptureLoop, if showHeaders is true, they only want to print headers too. But if a body is small and fits in one packet, this might print it inadvertently since it doesn’t know where headers end except by searching for double newline. Hopefully, the code handles it. - The output string will have the first line and, if included, headers separated by newline characters, exactly as they appeared (with CRLF normalized to newline in the string builder usage). - The method ensures it doesn’t go beyond end index (caplen in usage, which is length of the captured packet data).
Usage Scenario: - In CaptureLoop.StdoutSink.onHttpLine, they call this method to get a text representation to print. They include timestamp and addresses around it. - If showHeaders is false, only the start line (e.g., "GET /index.html HTTP/1.1") will be returned (plus maybe a newline at end). - If showHeaders is true, the returned string could include multiple lines: the request/response line and whatever headers fit in the limit.
Performance: - This is meant for human-facing output, so performance is not critical compared to other parts of the system. Still, it avoids extremely large output by capping at 4KB or 64KB. - It uses a StringBuilder and creates a few Strings. The major cost is decoding bytes to characters. It assumes the HTTP headers are ASCII (which is generally true; if not, it’s likely still fine to interpret in UTF-8 or ISO-8859-1). - The 64KB cap for headers is generous (typical headers are much smaller, but in case of very large cookies or payload in headers it stops at 64KB). - By truncating at these limits, it prevents the console from being flooded if something goes awry (like binary data without newline). - It doesn’t allocate more than necessary: just one builder and maybe one temporary string for the first line. This is fine for interactive use.
Extensibility: - One might add other formatting helpers. Currently, it only prints the first line (always) and optionally the rest of headers. - If needed, one could add a method to pretty-print the body in some way (like in hex or safe text) for debugging binary content. But that’s outside typical needs for HTTP text. - If the system needed to output to different logging systems or formats, one might adapt this to output a structured object or to use a logging framework. Right now it returns a plain String which is then printed. - The method could be extended to handle edge cases better, e.g., ensure it stops at the blank line after headers explicitly. If a future developer finds that it sometimes prints body data when showHeaders is true, they might add logic to detect the double CRLF and cut off. - The class is final with a private constructor (utility style), which is fine. New methods could be added if needed, e.g., to print just the first line or to parse the first line into components.
External Libraries: None. It just uses core Java (string handling). Charset default usage might be platform default – ideally they’d use StandardCharsets.UTF_8 explicitly to decode bytes, but if not given, it might rely on platform default charset (which could be an issue if not UTF-8). Given HTTP headers are ASCII, it usually won’t matter.
HttpEventSink.java
Summary: HttpEventSink is a simple interface defining a sink for complete HTTP messages. It represents a consumer that can accept an HttpMessage (as defined by the interface above) for processing or storage. This is part of the extensibility of the system – it allows plugging in different outputs for assembled HTTP messages.
Interface Definition: - It has a single method: boolean offer(HttpMessage m). - The method returns a boolean, presumably indicating if the message was accepted/queued successfully (true) or if it was dropped/rejected (false). This semantic is similar to queue offer operations. - The offer method suggests non-blocking submission – it might return false if a queue is full or the sink is not ready. A sink implementation could also always return true if it handles backpressure internally or can block (though by convention, offer implies a try without guarantee).
Usage in Code: - The HttpEventSink is meant for a mode where you handle whole messages. For example, a MessageFileSink implements HttpEventSink by writing each message to a separate file. - In HttpAssembler, there is a constructor that takes an HttpEventSink and a SessionIdExtractor. However, in the provided code, that constructor actually throws an IllegalArgumentException("Use streaming ctor"). This indicates the developers intended to deprecate/remove the event sink mode in favor of the streaming sink approach. They want users to use HttpStreamSink for performance. - Nevertheless, HttpEventSink remains as part of the SPI – likely to allow any legacy integration or simpler use-cases. - SegmentSink implements HttpEventSink as well (with an offer(HttpMessage m) method). In SegmentSink.offer, the implementation actually converts the message to streaming internally (calls its own begin, append, end for that message). They note this is a fallback for compatibility (in the code comment, they mention it's not used in the streaming path). - MessageFileSink also could implement HttpEventSink (given its purpose). It likely has an offer(HttpMessage m) that queues the message for writing.
Design Rationale: - Having this interface means the assembler could theoretically output either to an event sink or a stream sink. In practice, the stream sink is favored. But the interface provides a clear contract if someone wants to capture complete messages easily at the cost of memory. - It's minimal by design: just one method. There is no explicit close() or flush() in the interface, so those would be part of the implementing class if needed (e.g., MessageFileSink is AutoCloseable on its own).
Performance considerations: - Using an HttpEventSink means the system will construct full HttpMessage objects and hand them off. This involves allocating arrays for the entire message content. It’s much heavier than streaming, especially for large bodies. That’s why it’s not used in high-throughput mode. - If an implementation of offer blocks or is slow (like writing to a slow storage synchronously), it could back up the assembler. The boolean return allows for non-blocking rejection, but how the assembler would respond to a false return is not defined in our snippet. Likely, the assembler would drop the message or try again – but since they aren’t using this mode, they didn’t flesh out that logic. - In practice, SegmentSink.offer always returns true (since it internally just dispatches to its queue which is large). But if that queue were full, offer might return false, in which case an upstream might drop the message or pause. However, since streaming path is default, they don't hit this scenario.
Extensibility: - This interface can be implemented to integrate with other systems: - For example, one could implement HttpEventSink to push messages into a Kafka topic, or to an in-memory analysis engine, etc. - That would require enabling the assembler’s event-sink mode (which currently throws). One might modify HttpAssembler to allow the event sink path if needed: possibly by storing the legacyEventSink and on completing a message, constructing HttpRequest/HttpResponse and calling offer. - Because that path is currently disabled, an extender would have to change the code to use it. - The streaming architecture proved more efficient, so extending via HttpStreamSink is usually preferable. But HttpEventSink remains a straightforward contract for simpler needs. - In future, if one wanted to process data differently, they might remove the exception and allow this usage (with the caveat of performance). - Alternatively, a developer might treat HttpEventSink as deprecated and remove it altogether if it’s truly no longer needed.
External Libraries: None. It’s a plain interface. The design and naming (offer) is reminiscent of queue terminology, but that’s just design.
HttpStreamSink.java
Summary: HttpStreamSink defines the interface for the streaming output of HTTP messages. Instead of handling whole messages at once, it provides a way to handle the start, chunks of body, and end of messages. This is the interface that HttpAssembler uses in the high-performance path to emit data incrementally. SegmentSink is the primary implementation of this interface.
Interface Members: - Nested interface HttpStream: a marker (empty interface). This represents an ongoing HTTP message stream. Implementations will return an object of type HttpStream (for example, SegmentSink.StreamHandle) from the begin method, which is then passed to append and end to identify which message is being written. - SegmentSink.StreamHandle begin(SegmentSink.Meta meta, byte[] headers): - Signals the start of a new HTTP message. meta contains metadata (id, type, session, src/dst, initial timestamp, etc.). headers is a byte array of the raw start line and headers of the message. - The implementation should begin writing a new record (for instance, write the headers into a blob and note their offset, but not finalize yet). - It returns an HttpStream handle which will be used to reference this message until it’s closed. - void append(HttpStream handle, byte[] data, int off, int len): - Writes a chunk of the message body associated with the given stream handle. The off and len allow writing a portion of the byte array (so the method can be called with a large buffer and specify only part of it). - This may be called multiple times as more body data arrives. - Implementation should write these bytes to the blob output, sequentially, keeping track of how many bytes have been written for this message. - void end(HttpStream handle, long tsLast): - Signals that the message is complete. tsLast is the timestamp of the last byte of the message. - The implementation should finalize the record – e.g., determine the total body length written since begin, and write an index entry (including the timestamps, message metadata, offsets, lengths, etc.). - It can then close out any resources associated with that handle. The handle becomes invalid after this. - (There is no explicit cancel or abort method defined – if a message didn’t complete normally, how that’s handled isn’t in the interface. Possibly not needed because HTTP messages in this system either complete or are not recorded.)
Design and Interaction: - HttpAssembler uses these methods to output messages. For instance, when it finds end-of-headers: - It generates Meta info and calls streamSink.begin(meta, headerBytes), getting a handle. - As it receives body data in subsequent segments, it calls streamSink.append(handle, buffer, offset, length) for each piece. - When it determines the message is done, it calls streamSink.end(handle, tsLast). - This interface allows the output sink to manage its own threading or buffering. For example, SegmentSink.begin places an “OPEN” command in a queue, append places “APPEND” commands, and end places a “CLOSE” command, which a worker thread processes to do actual file I/O. Thus, the assembler thread doesn’t block on disk writes. - The HttpStream handle is opaque to the assembler; only the sink knows what it is (for SegmentSink, it carries an internal stream ID and maybe references to file positions).
Performance Considerations: - This streaming approach is what enables the system to handle very large bodies and high throughput. It avoids copying big buffers unnecessarily: - For instance, the headers are handed off as a byte array once, and body chunks as they come. The assembler doesn’t concatenate or store entire messages. - The sink implementation can write out data in large sequential chunks (which is efficient for I/O). - One must ensure that the append calls are in correct order for each handle and that end is called eventually. The interface relies on the contract from the producer (HttpAssembler) to call begin once, append zero or many times, then end once for each message. - Implementations should handle interleaved streams if it ever happened (though in HTTP/1, messages aren’t interleaved on the same connection; but the sink might see multiple open streams if using one sink for multiple assembler instances or something). In practice, in this design, at most one stream is open per connection at a time. - The interface doesn’t include any method to flush or sync explicitly, but an implementation like SegmentSink might flush internally after a certain number of messages, etc.
Extensibility: - One can implement HttpStreamSink to send data elsewhere. For example, you could implement a sink that writes to a network socket or pipes to a command. Or one that buffers in memory for analysis. - If implementing a new sink, one would likely mirror what SegmentSink does: create some structure on begin (like open a file or allocate a buffer), collect data on append, finalize on end. - The Meta class used by SegmentSink is not part of the interface, but SegmentSink.Meta is passed in; an alternative implementation could ignore it or use its data similarly. Since the interface is tied somewhat to SegmentSink.Meta, which is specific to that implementation, a custom sink might still accept it but only use the fields it cares about. - The interface could be extended with default methods if needed (for example, to handle some default behavior on certain conditions) but currently it’s minimal.
External Libraries: None. It’s a plain interface. The implementers (like SegmentSink) use core Java I/O and util packages.
MessageFileSink.java
Summary: MessageFileSink is an implementation of an output sink that writes each HTTP message to an individual file. It is a legacy or alternative to SegmentSink. When a message is completed, it writes the headers and body to a separate .http (or possibly .http.gz) file on disk. This approach is straightforward but can be inefficient at scale due to file count and overhead.
Key Components: - Config inner class: Holds configuration for the sink: - Path root – the directory in which to create files. - boolean gzip – whether to gzip-compress each file. - int queueCapacity – the capacity of the internal queue for messages. - A static factory or constructor sets these fields. (Defaults might be provided by the caller or a static method; e.g., perhaps a default not provided, requiring explicit config). - Queue and Worker: The MessageFileSink is likely an active component with a background thread: - It may have a BlockingQueue<HttpMessage> of given capacity. - Messages offered via offer(HttpMessage m) are put into this queue (possibly using offer() or put() depending on whether they want to block or drop). - A dedicated worker thread takes messages from the queue and writes them to disk. - This decouples the writing from the assembler thread, preventing delays in assembly. - File Naming: It probably names files using the message’s ID or a sequence number. The typical approach might be <id>.http or <id>.http.gz if compressed. Another approach could be timestamp-based or incremental numbering. The code would ensure uniqueness (the id is already unique). - If gzip is true, it likely appends “.gz” extension or similar. - Writing Logic: For each HttpMessage m: - It determines an output file path (something like root/(id)_(REQorRSP).http). Actually, since request and response share an ID, one might incorporate kind in the filename to distinguish if necessary, or possibly combine them? But since this sink writes one message per file, not pairs, it might label them with kind. - It opens a FileOutputStream (possibly wrapped in GZIPOutputStream if compression is on). - It writes the start line and headers (retrieved via m.rawStartLineAndHeaders()), then writes a separator (like an extra newline). - Then writes the body bytes (m.rawBody()). - Closes the file (flushing to disk). - Resource Management: The class implements AutoCloseable, meaning it has a close() method. In it: - It probably signals the worker thread to finish (maybe by sending a special sentinel through the queue or interrupting). - It waits for it to flush any remaining messages and properly close. - It might also handle shutting down of resources, though the main resource is file handles which are closed immediately after writing each message.
Interaction with Assembler: - If used, MessageFileSink would be passed as an HttpEventSink to HttpAssembler. The assembler would create HttpMessage objects and call offer() on this sink. - The offer(HttpMessage m) method here: - If the queue is not full, put the message and return true. - If it is full, possibly drop the message and return false (or block until space if implemented differently). - Because the assembler in current code doesn’t handle the false case explicitly (since it doesn’t use this path), it might be okay to block instead. But returning false would allow backpressure if integrated.
Performance and Bottlenecks: - Writing each message to its own file has significant overhead: - File creation and close for every single HTTP request/response. This could overwhelm a filesystem if messages are numerous (e.g., thousands per second would mean thousands of file create operations per second). - It also results in very large number of small files, which is hard to manage (why the new approach was devised). - The queue and background thread mitigate the immediate performance hit on capture, but eventually the disk I/O will be a limit. - Gzip option saves space but adds CPU overhead to compress each file; useful if storing large payloads and disk space is a concern, but it further slows down writing. - Memory: If messages are large, they reside fully in memory as HttpMessage until written. That can spike memory usage if many messages are in the queue or being processed slowly. The queueCapacity helps bound how many messages can pile up if writing can’t keep up. - For moderate volume or debugging, this sink is fine. For heavy, the system clearly favors SegmentSink.
Extensibility: - The concept here is straightforward, so extending might be in terms of naming convention (someone might want to name files by timestamp or include host/path in filename, etc.). Currently, the code likely just uses the id which is unique and sortable, maybe prefixed with timestamp. - One could also extend it to create subdirectories (if there are millions of files, you might partition by time or some hash to avoid directory overload). - Could also include response pairing logic, but since it handles messages individually, pairing is not directly done – that’s left to analysis outside (by matching IDs). - If a developer needed to quickly output some sample of traffic to easily inspect, enabling this sink could be useful. They might extend it with filtering (only write files for certain criteria) to reduce volume. - Another extension: Instead of writing raw bytes, it could format them nicely (like maybe adding some metadata or human-readable formatting). But likely it writes raw bytes as captured (which can be opened in a text editor or analyzed).
External Libraries: None beyond core Java I/O. Possibly uses java.util.zip.GZIPOutputStream for compression if enabled.
SegmentSink.java
Summary: SegmentSink is the primary implementation of HttpStreamSink for the system’s output. It is responsible for writing assembled HTTP messages into large “blob” files and maintaining an index of those messages in NDJSON format. It does this in a streaming, asynchronous fashion – meaning the assembly thread hands off message data to SegmentSink, and SegmentSink uses a worker thread to perform actual file I/O. The output of SegmentSink is a set of rolling files: - Blob files containing concatenated HTTP headers and bodies (binary data exactly as captured for each message). - Index file(s) in NDJSON (newline-delimited JSON) where each line corresponds to one HTTP message, with metadata (IDs, timestamps, etc.) and pointers (offsets/lengths) to where that message’s parts reside in the blob files.
Important Classes/Structures: - Config inner class: Contains configuration for how SegmentSink operates: - Path root – directory to place output files (the http-out directory). - long rotateBytes – size threshold for rotating to a new blob file (e.g., 1 GiB). - long rotateSeconds – time threshold for rotation (e.g., 3600 seconds for hourly). - int queueCapacity – how many operations can queue up (like how many messages or chunks) before blocking; default given is 32768. - int batch – a batch size for processing queue items (e.g., 512) – likely the worker will pull up to this many operations in one go to write in batch for efficiency. - boolean gzipIndex – whether to gzip-compress the index files or not. - A static helper hourlyGiB(Path root) provides a preset config: 1 GiB or 1 hour rotation, large queue, batch 512, and not gzip index (as used in AssembleMain).
Meta inner class: Represents metadata for a single HTTP message:
Fields: id (transaction ID), kind ("REQ" or "RSP"), session (session ID if any or empty), src, dst (IP addresses), tsFirst (timestamp of first byte), tsLast (timestamp of last byte; filled later).
Also firstLine – not filled in constructor, but once headers are processed, they might store the start line for logging or potentially for index (though the index likely doesn’t include the entire first line; maybe for debugging).
The Meta is provided by assembler on begin call. It doesn’t include ports, which is interesting (maybe src and dst include ports as part of string? Or ports omitted? Possibly omitted since not crucial for identifying messages uniquely, though they could be logged).
StreamHandle inner class: Implements HttpStreamSink.HttpStream. It has a field long sid – a stream ID (sequence number for messages).
The sid is likely a monotonically increasing counter for messages handled by this SegmentSink. It might be used internally for tracking open streams.
The handle doesn’t carry much else; it’s just an identifier. The implementation might not even use sid except perhaps for logging or debugging. More likely, the actual tracking of where to write is in the worker’s state rather than in the handle.
Worker Thread and Queue (Internals):
BlockingQueue<Cmd> q – the sink uses a queue of command objects (or an enum + data) representing actions: perhaps OPEN, WRITE (append), CLOSE for messages. Each command likely encapsulates the data needed (e.g., for open: Meta + headers; for append: handle + bytes; for close: handle + tsLast).
The sink starts a dedicated thread (worker) in the constructor, which runs drainLoop().
drainLoop() continuously takes commands from q and executes file I/O accordingly.
The use of ArrayBlockingQueue with capacity (e.g., 32768) means if assembler is far ahead of disk writes, the queue can fill up. If it fills, the assembler thread (on offer or internally begin/append) will block until space frees (thus providing backpressure rather than dropping data).
The batch config likely means the worker will take up to batch commands from the queue at once and process them in a tight loop before flushing. This amortizes lock overhead and allows coalescing writes.
RollingFiles private inner class: Manages the actual file handles for blob and index:
It tracks the current blob FileChannel and index OutputStream (could be FileOutputStream or GZIPOutputStream).
maxBytes and maxAgeSec from config to decide when to rotate.
Fields like blobSize (bytes written so far in current blob) and openedAtSec (timestamp when current files opened).
It has methods to maybe open a new file (nextFile() or similar) when rotation criteria met.
Likely naming convention for files:
Blob files might be named like blob-<timestamp>-<counter>.bin or .blob.
Index files like index-<timestamp>-<counter>.ndjson (or .ndjson.gz if compressed).
The use of a timestamp in name ensures sort order and uniqueness. The part counter in SegmentIO was per rotation within the same timestamp second; maybe similar here.
On rotation: close current files, open new ones, write a header or magic if needed (SegmentIO had a magic, but here because index is text and blob is self-delimited by index, they might not need a magic number – though they might not include one, or they could).
Possibly writes a “MAGIC” at top of blob file for identification, but in code we see none referenced in SegmentSink, maybe not.
Assembling Index Lines: When a message ends, the worker, upon processing the CLOSE command:
It knows the Meta (id, session, etc.) from begin, the total bytes of header and body (since it wrote them), and their offsets.
It constructs a JSON line string with those fields. For example:
{"id":"01FC8XYZ...","ts":163102...,"te":163102...,"src":"1.2.3.4:1234","dst":"5.6.7.8:80","session":"","kind":"REQ","hdr_off":123456,"hdr_len":345,"body_off":123801,"body_len":789}
The actual keys might differ (and they may include method or status too if parsed, but likely not in this index, to keep it lighter).
It writes that line to the index file (with a newline).
It also decrements the count of active streams, etc.
Flush Policy: The code snippet shows that after a certain number of index lines written (sinceFlush >= 2048), it flushes the index file and blob.force(false) (fsyncs the blob) to ensure data is persisted. This is a trade-off between throughput and durability. Flushing every 2048 messages means minimal performance loss but bounds data loss if crash to at most 2048 messages (plus whatever OS buffered for blob).
sinceFlush resets after flush. So, on a busy system, it will periodically flush in chunks.
On close (SegmentSink.close() or on rotation), it would flush and close as well.
Compatibility Methods:
offer(HttpMessage m) implementation in SegmentSink (since it implements HttpEventSink too) internally just uses the streaming API:
It creates a Meta from the message (id, etc.), calls begin, then append with the entire body, then end. So it essentially converts a whole message into the stream flow. It returns true if all went well. This way, if someone uses HttpEventSink interface (not recommended), it still funnels into the same mechanism.
Putting it together (Example workflow for one message): 1. Assembler calls begin(meta, headersBytes): - SegmentSink.begin creates a new Meta (or uses provided one, probably uses it directly). It assigns a new sid (from nextSid counter) and creates a StreamHandle with that ID. - It enqueues an OPEN command containing (sid, Meta, headersBytes). - Returns the StreamHandle (with sid). 2. As assembler feeds body: - For each chunk, SegmentSink.append(handle, data, off, len) enqueues an APPEND command (sid, bytes fragment). - It might copy the bytes to avoid later mutation issues, or perhaps not if assembler ensures to not reuse the buffer (but safer would be to clone or slice). 3. When assembler finishes message: - SegmentSink.end(handle, tsLast) enqueues a CLOSE command (sid, tsLast). - It does not return anything, just void. 4. Meanwhile, the worker thread is reading commands: - On OPEN: it possibly rotates file if needed (check current size/time vs thresholds). Then opens new if needed, else continue. * It writes the header bytes to the blob file. It notes the offset of where headers started (e.g., current blob position before writing). * It records internally the mapping from sid to a partial index entry: perhaps it stores in a map like currentMessages[sid] = {meta, hdrOff, hdrLen} for use when closing. * It increments activeStreams count (maybe to ensure no rotation mid-message). - On APPEND: it writes the data bytes to blob at current position. If multiple APPENDs, they are just sequentially written. * It may accumulate body length for that sid in the partial info. - On CLOSE: it finds the info for that sid (Meta and offsets/lengths). * Calculates hdr_len (which was known right after open as the header byte count). * Calculates body_off (which is header offset + header length; or simply the offset after header in blob). * Calculates body_len as total bytes written for this sid minus header length. * Fills tsLast in the Meta. * Forms the JSON line and writes to index. * Deallocates that sid’s entry (remove from map). * Decrement activeStreams. If activeStreams becomes 0 and either time or size threshold reached, it could rotate file at a safe point (ensuring no open messages). - The worker also checks rotate conditions on each command maybe, but importantly, rotation is deferred until no message is in-flight. That is, it will not cut a file in the middle of a message. This guarantee ensures each message’s header and body reside entirely in one blob file (and correspondingly one index file). - The RollingFiles class likely handles the actual closing and opening of new files.
Performance Considerations: - Asynchronous disk I/O: The separation of threads allows network reading/assembly to proceed without waiting on disk writes. - Large sequential writes: Blob file writes are buffered (1MB buffer on stream) and sequential, which is optimal for throughput on disk. Writing a huge series of HTTP bodies end-to-end is essentially replicating a pcap’s role but at HTTP layer. - Minimizing file handles: Only one blob and one index are open at a time per SegmentSink (which is per assembly process). - Index writing overhead: Each message adds a JSON line (a few hundred bytes maybe). That’s a minor overhead relative to body sizes typically. Searching in text is slower than binary index, but they opted for NDJSON for usability (can grep by ID or session easily). - Memory: The queue can hold many operations; worst-case memory is if lots of data is queued (each command might hold a byte array slice). But queueCapacity=32768 and batch writes mitigate extreme backlog. If each command was, say, 2KB, that’s up to ~64MB of data queued, which is within reason and aligns with OOO global limit, etc. - The batch parameter ensures the worker will try to take up to 512 commands at once from queue and handle them in one loop iteration. This improves throughput by reducing context switch overhead and allowing grouping of small writes possibly into one larger flush. - Because the drainLoop might handle multiple messages before flushing, in a crash scenario a few thousand messages might be buffered not yet sync’d. They accepted that risk for performance.
Extensibility: - If wanting to change the format or fields in index, one can adjust the JSON assembly in the code. For example, one might add the HTTP method or status code to index lines by parsing the Meta.firstLine. Currently, they did not do that, likely to keep index smaller. They favored storing minimal info needed to retrieve the raw message (IDs, times, offsets) and use external tools to parse content if needed. But one could extend to include method, URL, status, content-type, etc., at cost of parsing in assembler and larger index. - The sink could be extended to optionally compress blob files if needed (though that would make random access via offset more complex, so likely not). - Gzipping the index is already an option (for saving space if needed). - The rotation policy is configurable. A developer might add logic to rotate also on index file size or number of records, though time and size of blob are usually enough triggers. - The class is tightly coupled with file system output, but one could imagine implementing a variant that writes to a database or cloud storage by changing how RollingFiles works. That would be a significant change though. - For multi-thread assembly, multiple SegmentSink instances (or sharded by flows) could be used, but then merging the index might be needed. The design currently expects one assembler -> one SegmentSink -> one set of output files.
External Libraries: None. All file I/O is via java.nio.file and java.io (FileChannel, streams). Concurrency via java.util.concurrent. JSON is constructed manually (no external JSON library), as evidenced by writing to streams directly with UTF-8 bytes. This avoids dependency and likely faster than using a JSON serializer for this simple task.
SessionIdExtractor.java
Summary: SessionIdExtractor is a utility class used to identify session or user identifiers in HTTP headers. It scans the headers of HTTP requests and responses for certain patterns (like session cookies or custom header values) and extracts a token that can represent a user’s session or identity. The extracted session ID (if found) is then attached to the Meta of a message, so that analysts can correlate messages by session.
Configuration: - The constructor takes two collections: - Collection<String> cookieKeys – names of cookies that should be treated as session identifiers. - Collection<String> headerRegexes – regex patterns to match in header lines for session IDs. - It stores these internally as: - A Set<String> cookieKeys (all lowercased for case-insensitive matching). - A List<Pattern> headerPatterns (compiled regex Patterns, case-insensitive).
Methods: - String fromRequestHeaders(Map<String,String> headers): - Iterates through each header entry in the provided headers map. - For each header, it constructs a full line string Key: Value and tests it against each regex in headerPatterns. If any pattern matches, it takes the last capturing group (m.group(m.groupCount())) of the regex match as the session ID and returns it, prefixed with "HDR:" to indicate it came from a header. * The idea of last capturing group is likely that the regex can have parts and the actual ID is in a capturing group at the end. E.g., a pattern might be Authorization: Bearer (.+) so the token is group1. - If none of the regex patterns matched in any header, it then looks for a "cookie" header (case-insensitive) in the map. * If found, it splits the cookie header value by ; (cookie delimiter). * For each cookie pair, splits at = to get name and value, trims and lowercases the name, and checks if it’s in the cookieKeys set. * If a match, it returns "SID:" + the cookie value (trimmed) as the session ID. - If still nothing found, returns null (meaning no session ID identified in this request). - String fromSetCookie(List<String> setCookies): - This is for responses. It takes a list of Set-Cookie header strings (each string typically like Key=Value; Path=/; HttpOnly etc). - It iterates through each Set-Cookie string: * Finds the first = to separate cookie name and value. * Lowercases and trims the name, checks if it’s in cookieKeys. * If match, it then takes the cookie value up to the next ; if present (i.e., the raw value of the cookie, without attributes). * Returns "SID:" + value trimmed. - If none of the cookies match the keys, returns null. - The idea is: session IDs might be in: - A specific cookie (like JSESSIONID or SESSIONID, etc.) in requests (as clients send cookies) and in responses (Set-Cookie issuing a new session id). - Or in a header (like an Authorization token or custom header). - The extracted ID is prefixed with "SID:" or "HDR:" to denote source, which can be useful in logs to differentiate what kind of token it was. (They might have chosen "SID:" for cookies and "HDR:" for header patterns).
Usage in System: - A single SessionIdExtractor is created (in our usage, with no patterns, meaning it’ll always return null). - HttpAssembler likely calls: - sessionX.fromRequestHeaders(headersMap) when a request’s headers are parsed, and stores that in the Meta.session if not null. - sessionX.fromSetCookie(setCookiesList) when a response’s headers are parsed, and stores that if not null. Possibly, if a session was already identified in the request of this transaction, they might keep that; or if none in request but the response set one, they’ll label the response with it. - The session ID in Meta is then written in the index line for both request and response, enabling grouping by session post-fact. - Even though in our provided code AssembleMain uses an empty SessionIdExtractor (so it’s effectively off), the mechanism is there. A user could configure it by providing patterns or cookie names. For example: - Cookie keys: JSESSIONID, SessionId, etc. - Header regex: maybe something like X-User-Id: (\w+) or Authorization: Bearer (.+) if tokens need tracking.
Regex and Matching: - The regex matching scans the entire "Key: Value" line. That means one can write flexible patterns, for instance: - If wanting to capture a query parameter in a header value: e.g., Referer: .*session=([^&]+) could capture a session ID in a URL query. - They specifically check each header line string with each pattern, which is O(N * M) for N headers and M patterns. Typically N and M are small (few dozens), so performance is fine. - Patterns are compiled case-insensitive, and cookie keys are lowercased, so everything is case-insensitive.
Performance: - In normal use, this runs once per message, which is fine. Regex matching on maybe a dozen headers is trivial load compared to I/O. - If patterns or cookie list are huge (say hundreds of keys or very complex regexes), it could slow down header processing a bit, but usually it’s a small set. - The biggest cost could be building the headers map that HttpAssembler needs to call these. That implies parsing all headers into a Map. That itself is some overhead (string allocations). They likely accepted that since SessionId extraction is optional and in typical usage maybe not used unless needed. When off, they pass empty lists so it does nothing quickly.
Extensibility: - The design is flexible: you just configure with different keys/patterns without code changes. - If needed, one could add more extraction logic. For example, maybe session IDs in the URL path or query: currently not handled because it only looks at headers. But one could feed the URL (path/query) as a pseudo-header into fromRequestHeaders by adding a synthetic entry or by extending this class with a method to parse URI. - Could also consider extracting from response bodies (like an HTML form token) but that’s beyond scope and not typical for session ID extraction in this context. - The prefixes "SID:" and "HDR:" are hardcoded. These are just markers. If needed one could unify them or change them, but it’s mostly for human identifying how it was found.
External Libraries: Uses java.util.regex.Pattern. No external libs. The regex is standard Java regex.
Bytes.java
Summary: Bytes is a small utility class providing static methods to treat Java bytes as unsigned and to assemble multi-byte sequences from a byte array. It’s basically convenience functions for reading network-byte-order integers from a byte array.
Methods: - static int u8(byte[] a, int off): returns a[off] & 0xFF. This converts the signed byte value to an unsigned 0–255 range int. - static int u16be(byte[] a, int off): returns a 16-bit big-endian value from the array: - Equivalent to (u8(a, off) << 8) | u8(a, off+1). - So it composes two bytes into an unsigned 16-bit integer. - static int u32be(byte[] a, int off): returns a 32-bit big-endian value from the array: - It composes four bytes: (u8(a,off) << 24) | (u8(a,off+1) << 16) | (u8(a,off+2) << 8) | u8(a,off+3). - Returns it as an int. (If the actual value doesn’t fit in signed 32-bit, well, in Java int is signed but the bits just roll over – but IP lengths etc. fit in 32 bits). - There’s no u64 in this class, presumably not needed for this context. - The class is final and has a private constructor, indicating pure static usage.
Usage: - These methods are used in Decoders and possibly in other low-level parsing code: - Decoders.isIpv4 uses u16be(p, 12) to check EtherType. - Decoders.ipv4HeaderLen uses u8(p, ipOff) to get the first byte. - TcpDecoder did not use these static methods (it manually did similar logic with Byte.toUnsignedInt and shifts). - Possibly other places like reading a capture file or Napatech adapter (if any) would use it, but we didn’t see other uses beyond Decoders. - It basically wraps repetitive bit-manipulations to make code more readable.
Performance: - These are one-liners that likely get inlined by the JIT. They are trivial bit ops. - Using these avoids having to create ByteBuffer or DataInputStream just to get ints, which is good (no allocations, no big-endian conversion overhead beyond bit shifts). - They assume the caller ensures there are enough bytes from off (no bounds check except what the array access inherently does which will throw if out of range). - Simplicity ensures minimal overhead.
Extensibility: - If needed, could add u32le, u16le for little-endian (not needed in network context here). - Could add u64be if dealing with 64-bit values, but currently not used (could be for timestamp or something, but not here). - Could add methods to get, say, an IPv4 address as an int rather than string (but they directly form strings in decoder). - Overall, it’s a very small utility that covers the needed cases.
External Libraries: None. It’s pure Java bit manipulation.
Pcap.java
Summary: Pcap is an interface that abstracts packet capture operations. It allows the rest of the code to be independent of the capture mechanism (libpcap, a PCAP file, etc.). It defines how to open a live capture and how to retrieve captured packets. This interface is the entry point for capturing network packets.
Methods: - PcapHandle openLive(String iface, int snapLen, boolean promiscuous, int timeoutMs, int bufferBytes, boolean immediate) throws PcapException: - Opens a live capture on the given network interface. - snapLen: maximum bytes to capture per packet. - promiscuous: whether to enable promiscuous mode (capture all traffic vs. only traffic destined for this host). - timeoutMs: read timeout in milliseconds for capture (libpcap uses this for buffering). - bufferBytes: size of the kernel buffer for capture (to handle bursts). - immediate: if true, use immediate mode (deliver packets as soon as they arrive, without waiting for buffer fill or timeout). - Returns a PcapHandle which can be used to fetch packets. - Throws PcapException on failure (e.g., if the interface can’t be opened, or permission denied, etc.). - String libVersion(): returns the version string of the underlying capture library (e.g., libpcap version). This is useful for logging and ensuring compatibility.
Nested interface PacketCallback: This defines the callback for packet arrival:
Method void onPacket(long tsMicros, byte[] data, int capLen).
It provides timestamp (in microseconds), the raw packet bytes, and the captured length (which might be <= packet length if truncated by snapLen).
This nested interface allows using Java 8 lambdas or method references to handle packets, which is convenient. The system’s capture loop uses this in constructs like handle.next((ts, data, len) -> { ... processing ... }).
Design and Usage: - Pcap is implemented by JnrPcapAdapter in this code. Conceivably, other implementations could be provided, for example: - A dummy one for testing that reads from a file or generates packets. - An adapter for a specialized hardware capture card (like Napatech or DPDK-based capture) could also implement Pcap. - The interface isolates libpcap specifics: e.g., the code using it doesn’t need to call pcap_next_ex or manage error strings; that’s handled in the adapter. - The PcapHandle returned is then used by CaptureLoop or CaptureMain to loop receiving packets.
External API and Integration: - The naming and parameters mirror libpcap’s pcap_open_live and related calls (including setting buffer size, immediate mode, etc.), which suggests it was designed to closely wrap libpcap functionality. - libVersion() in implementation calls pcap_lib_version() to fetch the version string of libpcap (for logging at startup).
Performance: - The design using a callback allows the capture loop to be in Java rather than requiring a native loop or continuous polling by returning packets one by one. The adapter likely uses pcap_next_ex in a loop internally, but here it’s one packet at a time with a callback for clarity. - There’s a minor overhead in each callback invocation (one extra interface call, plus lambda capturing overhead if any), but it’s negligible relative to overall cost of copying packet data from native to Java (which is the major overhead). - Using a functional interface for PacketCallback means we can use lambdas, which is a modern and succinct approach.
Extensibility: - If we wanted to support offline pcap files, one might add a method like openOffline(String pcapFile). But since it’s not in interface currently, one could either extend interface (creating a child interface) or just handle offline differently outside this abstraction. - Additional capture options (like setting filters at open time, or selecting monitor mode for wireless) are not included here, except filter is applied later via PcapHandle.setFilter. - The Pcap abstraction could also be implemented by a replay mechanism (reading .segbin or .pcap files and feeding into the system for testing). Currently, not implemented but possible.
External Libraries: This interface by itself doesn’t use an external lib. But it is intended to be implemented by something like JNR which calls libpcap. So indirectly, JnrPcapAdapter uses the JNR-FFI library and libpcap native library. The interface is library-agnostic.
PcapHandle.java
Summary: PcapHandle is an interface representing an open capture session. It provides methods to control the capture once opened – specifically, applying a filter, capturing packets (with a callback), and closing the handle. This interface is returned by Pcap.openLive and used to retrieve packets.
Methods: - void setFilter(String bpf) throws PcapException: - Applies a BPF (Berkeley Packet Filter) filter to the capture. The filter string is in libpcap syntax (e.g., "tcp port 80"). - Throws PcapException if filter compilation or setting fails. - Under the hood, implemented via pcap_compile and pcap_setfilter. - boolean next(Pcap.PacketCallback cb) throws PcapException: - Captures the next packet (or waits until one is available or timeout occurs). - It invokes the provided PacketCallback’s onPacket method when a packet is captured. - Returns true if a packet was captured or the call timed out but can be called again. Returns false if it reached end of capture (EOF for offline files or if interface closed). - For live capture, it will almost always return true (unless an error or it's closed). - If an error occurs (other than timeout), it throws PcapException. - Essentially, this wraps pcap_next_ex: returning false corresponds to PCAP_ERROR_BREAK or EOF. - @Override void close(): (since it extends AutoCloseable). - Closes the capture handle, cleaning up native resources. After calling this, no further packets can be captured.
Usage in Code: - After obtaining a PcapHandle, the typical usage is:
try (PcapHandle handle = pcap.openLive(...)) {
    if (filter != null) handle.setFilter(filter);
    while (handle.next((ts,data,caplen) -> { ... process ... })) {
        // loop until next returns false (which for live likely never unless break)
    }
}
The try-with-resources ensures close() is called at end. - CaptureLoop.runForever and CaptureMain.main both follow this pattern.
Design considerations: - The callback approach in next avoids exposing raw pcap internal structures to Java, and avoids returning some complex object per packet (they just use a byte array). - Not providing a direct method to get Packet object keeps it simple (fewer allocations). It just gives the byte array and length. - It’s synchronous: next will block the thread until a packet arrives or timeout occurs. This fits libpcap’s model (non-polling without extra threads). - The handle is one-time use for one capture session. If wanting to capture concurrently from multiple interfaces, you open multiple handles (which is fine as long as concurrency is handled by the library and different threads).
Performance: - The implementer (JnrPcapAdapter.HandleImpl) does heavy lifting: copying bytes from native to Java with minimal overhead and then calling the callback. - The next call likely corresponds to one iteration of reading a packet. It’s a one-packet-per-call design (which may be slightly less efficient than a batch capture approach, but libpcap’s API is generally packet-at-a-time anyway unless using pcap_loop). - The overhead in Java per packet: the lambda call and whatever processing done. That’s usually fine relative to I/O times. - Because of how libpcap works, after timeoutMs without a packet, next returns true (with n==0 internally, just a timeout) and calls callback 0 times (meaning our lambda is not invoked, but next returns true because it didn’t hit EOF). Actually, in our adapter implementation, they handle n==0 by returning true (continue looping). - So the loop spins on timeouts. But because of blocking read in pcap, it’s not a tight busy loop; it’s just waking every timeout interval. With immediate mode, it wakes as soon as a packet is there (or at least returns with each packet).
Extensibility: - Could add additional methods if needed, e.g., statistics (packets dropped etc. via pcap_stats). Not present now. - For offline file support, one could implement next such that it returns false when EOF reached (which is already in logic). - If wanting to allow injection of packets (pcap_sendpacket), that would be beyond this interface’s scope (would likely be in Pcap or a separate interface). - As currently stands, it’s minimal and sufficient for reading.
External Libraries: The interface itself uses none, but implementations rely on JNR and native libpcap. The user of this interface doesn’t need to know that. It just deals with exceptions and the data.
PcapException.java
Summary: PcapException is a custom exception class for errors related to packet capture operations. It extends Exception (a checked exception).
Constructors: - PcapException(String msg): create with an error message. - PcapException(String msg, Throwable cause): create with message and underlying cause.
Usage: - It’s thrown by methods in Pcap and PcapHandle to signal errors: - If openLive fails (e.g., invalid interface or no permission), the adapter throws new PcapException("pcap_create failed") or with specific libpcap error string. - If setFilter fails (e.g., invalid filter syntax or other error from libpcap), it throws a PcapException with the error message (maybe including pcap_geterr output). - If next encounters an error (libpcap returns -1), they throw PcapException with pcap_geterr output. - So any critical libpcap operation results in this exception on failure. - Being a checked exception, callers either catch it or declare it. In our code, CaptureMain.main declares throws Exception (so it doesn’t catch, it will propagate causing program exit with stack trace if an exception happens). - CaptureRunner.main catches some usage error (like missing iface) by printing usage, but if pcap.openLive fails, it would throw and likely not caught (so it would print an exception on stderr). - Possibly, a more user-friendly handling could be added by catching PcapException specifically to print a user-friendly error (like "Failed to open interface: ...").
Design: - It’s a final class, simple structure. It doesn’t add any specific methods beyond what Exception has. - Marked as checked (since extends Exception, not RuntimeException). This is good because capture errors are something you might want to handle at a higher level (like try a different interface or inform user), rather than silently ignoring.
Performance: - Only relevant in error conditions; creation of exception involves filling stack trace etc., which is fine given it’s exceptional.
Extensibility: - If needed, more specific exception types could extend PcapException (for example, FilterCompileException, PermissionDeniedException), but likely overkill. Simpler to use the message. - Possibly they could have included error codes or constant messages, but libpcap primarily provides text error messages, so it’s simplest to pass those through.
External: None in itself (just extends Exception). It is thrown when underlying native calls fail, but that coupling is via messages only.
JnrLibpcap.java
Summary: JnrLibpcap is an interface for JNR (Java Native Runtime) that defines the mapping of libpcap’s native functions to Java methods. Using JNR’s annotations and method signatures, it allows calling libpcap functions as if they were Java methods.
Notable Mappings (from snippet and known libpcap API): - INSTANCE = LibraryLoader.create(JnrLibpcap.class).load("pcap"): This loads the libpcap library and returns an implementation of this interface linked to the native calls. - Methods: - Pointer pcap_create(String source, Pointer errbuf): corresponds to pcap_create, which prepares a pcap handle for later activation. Returns a pcap_t * as Pointer. - int pcap_set_snaplen(Pointer p, int snaplen) - int pcap_set_promisc(Pointer p, int promisc) - int pcap_set_timeout(Pointer p, int to_ms) - int pcap_set_buffer_size(Pointer p, int bytes) - int pcap_set_immediate_mode(Pointer p, int on) - int pcap_activate(Pointer p) - These correspond to setting various options and activating live capture. - int pcap_compile(Pointer p, BpfProgramStruct prog, String filter, int optimize, int netmask) - Compiles a BPF filter. BpfProgramStruct is a JNR mapping of struct bpf_program that will be filled by libpcap. - int pcap_setfilter(Pointer p, BpfProgramStruct prog) - Applies the compiled filter to the capture handle. - void pcap_freecode(BpfProgramStruct prog) - Frees the memory allocated by pcap_compile for the filter program. - int pcap_next_ex(Pointer p, Pointer pkt_header_pp, Pointer pkt_data_pp) - This is crucial: it fetches the next packet. It returns 1 for a packet, 0 for timeout, -1 for error, -2 for EOF (if reading from file). - It populates pkt_header_pp and pkt_data_pp with pointers to the packet header and data. - String pcap_geterr(Pointer p) - Returns the last error text for the given handle. - void pcap_close(Pointer p) - Closes the capture handle. - String pcap_lib_version() - Returns libpcap version string. - The cstruct package classes (TimeVal, PcapPkthdr, BpfProgramStruct) are used in conjunction: - PcapPkthdr in Java corresponds to struct pcap_pkthdr which includes a struct timeval ts; bpf_u_int32 caplen; bpf_u_int32 len;. - JNR allows to map these and even reuse them for reading values (they do H.useMemory(hdrPtr) to map the native header). - BpfProgramStruct corresponds to struct bpf_program (with bf_len and bf_insns fields). - TimeVal corresponds to struct timeval (with tv_sec and tv_usec). - The Memory.allocateDirect(rt, 256) likely was used to allocate errbuf pointer for pcap_create.
Design: - This interface simply declares the native signatures. JNR will implement them at runtime. - The parameter and return types are either primitives or JNR-provided Pointer or Struct classes, which JNR knows how to handle. - The use of JNR means no explicit JNI code; it’s dynamically linked and calls through libffi.
Usage in JnrPcapAdapter: - They call JnrLibpcap.INSTANCE.pcap_create(iface, errbuf), check for null, etc. - Then a series of pcap_set_* calls, then pcap_activate. - For reading: - They allocate two Pointer objects to hold pointer-to-pointer for header and data (hdr_pp, data_pp). - On pcap_next_ex, pass those pointers; libpcap fills them with addresses of the packet header struct and packet data. - Then they map PcapPkthdr to that header pointer to read the fields easily in Java (via the struct class). - pktPtr.get( ... ) is used to copy the packet data into Java array.
Performance: - JNR calls have some overhead (but less than JNI to develop at least; at runtime they might be a bit slower than a hand-written JNI as they go through libffi). But in context, copying packet data likely dominates. - For each packet, they call pcap_next_ex (native), then a few JNR struct field accesses. That’s okay given likely tens of thousands of packets per second at most typical scenario. Could handle even more with tuning, but beyond that maybe consider zero-copy or JNI. - JNR is a pragmatic choice: development speed over absolute performance. It’s likely sufficient for 1-10 Gbps with efficient Java, but at extreme rates, JNI or even moving capture out-of-process could be needed. The comment indicates they achieved 10 Gbps with this design due to focusing on zero per-packet allocations.
Extensibility: - If libpcap API updates (like new functions), they can be added here. - If wanting to support pcap_open_offline, they’d add Pointer pcap_open_offline(String filename, Pointer errbuf) mapping. - Additional utility functions like pcap_datalink (for link type), pcap_stats could be mapped if needed. - The interface approach is convenient; you could also have multiple instances (like if wanting different libpcap versions via different names, but here one global INSTANCE is fine).
External Libraries: JNR-FFI (which is an external library). But JnrLibpcap.java itself is just an interface using JNR annotations and infrastructure.
JnrPcapAdapter.java
Summary: JnrPcapAdapter is the class that implements the Pcap interface using the JNR libpcap binding (JnrLibpcap). It encapsulates the logic for configuring and starting a libpcap capture and provides a nested HandleImpl class that implements PcapHandle. This is the concrete adapter connecting our Java code with native packet capture.
Key Fields: - private final JnrLibpcap p = JnrLibpcap.INSTANCE; - This is the singleton through which all libpcap calls are made. - private final Runtime rt = Runtime.getSystemRuntime(); - JNR’s runtime, used for memory allocation and linking with JNR Structs.
Methods implementing Pcap: - openLive(String iface, int snap, boolean promisc, int timeoutMs, int bufferBytes, boolean immediate): - It uses libpcap’s modern API (pcap_create + set options + activate) instead of the older pcap_open_live for fine control. - It allocates an error buffer (256 bytes) using JNR’s Memory (direct memory). - Calls pcap_create(iface, errbuf). If it returns NULL, it means failure (err message in errbuf). It then throws PcapException with a message. - On success, it gets a Pointer ph (pcap handle). - Then calls sequence of pcap_set_snaplen(ph, snap), pcap_set_promisc(ph, promisc?1:0), pcap_set_timeout(ph, timeoutMs), pcap_set_buffer_size(ph, bufferBytes), pcap_set_immediate_mode(ph, immediate?1:0). * For each, it calls a check(ph, resultCode, "functionName failed") likely (the snippet beyond 500 shows check(ph, pcap_set_snaplen, ...)). This check method probably throws exception if resultCode < 0 (which means error in libpcap). - Then calls pcap_activate(ph). If that returns negative (error), it gets error message via pcap_geterr(ph) and throws PcapException, also closing the handle. - On success, it returns a new HandleImpl(ph, ...) – an instance of the inner class implementing PcapHandle. - libVersion(): - Simply calls p.pcap_lib_version() and returns the String (libpcap returns a static string like "libpcap version X.Y.Z").
Inner Class HandleImpl (implements PcapHandle): - Fields: - Pointer ph; – the native pcap handle. - Runtime rt; – JNR runtime (likely same as adapter’s). - JnrLibpcap p; – reference to the JNR libpcap instance for convenience. - Some objects for efficiency: * Possibly a PcapPkthdr H; (struct mapping) to reuse for reading header fields. * Pointer hdr_pp, data_pp; – pointers allocated to hold pointer results from pcap_next_ex. * ThreadLocal<byte[]> SCRATCH; – a thread-local buffer for packet data, initial size 2048 bytes, that grows as needed up to snaplen. This avoids new byte array per packet; reuse the same array (growing it if a larger packet arrives). - Constants: * COPY_CHUNK = 4096; – they copy the packet data in 4KB chunks from native to Java array to reduce JNR overhead (multiple smaller calls vs one large? Actually, they do in a loop to avoid one JNR call per byte). * SCRATCH_INIT = 2048; – initial buffer size.
setFilter(String filter):
They allocate a BpfProgramStruct prog (JNR struct for compiled filter).
Call p.pcap_compile(ph, prog, filter, 1, 0) (optimize=1, netmask=0 – netmask 0 means treat "host" addresses in filter as IPv4 addresses with no netmask context).
If result is non-zero, throw PcapException with p.pcap_geterr(ph).
If compiled, call p.pcap_setfilter(ph, prog). If that fails, throw exception similarly.
Finally call p.pcap_freecode(prog) to free allocated filter instructions.
(If any step fails, likely they free code if needed and throw).
next(PacketCallback cb):
This is the core capturing loop method.
Calls int n = p.pcap_next_ex(ph, hdr_pp, data_pp).
If n == 1 (packet captured):
It gets Pointer hdrPtr = hdr_pp.getPointer(0) and Pointer pktPtr = data_pp.getPointer(0).
If pktPtr is null or address 0 (shouldn’t normally happen unless maybe an empty packet?), they just return true to continue loop (defensive).
They use the PcapPkthdr H struct:
H.useMemory(hdrPtr) to point it at the native header data.
Then long tsMicros = H.ts.tv_sec.longValue() * 1_000_000 + H.ts.tv_usec.longValue(); – computing timestamp in micros.
int caplen = H.caplen.intValue();
If caplen < 0, set to 0 (just a guard, normally not negative).
If caplen > snaplen, clamp to snaplen (shouldn’t happen because libpcap shouldn’t deliver more than snaplen, but they double-check).
Ensure the scratch buffer is large enough:
Get the current byte[] buf = SCRATCH.get().
If buf.length < caplen, then increase buffer size:
Double it until it’s >= caplen or hit snaplen. Ensure not to exceed snaplen.
They cap growth at snaplen since that’s max possible captured bytes.
Use Arrays.copyOf to allocate a new bigger array and set SCRATCH to it.
This growth is geometric and will happen rarely (only when a packet larger than previous max is seen, up to snaplen).
Copy data from native memory to Java byte array:
They copy in a loop:
int remaining = caplen;
int off = 0;
while (remaining > 0) {
    int chunk = Math.min(remaining, COPY_CHUNK);
    pktPtr.get(off, buf, off, chunk);
    remaining -= chunk;
    off += chunk;
}
This breaks the copy into 4KB chunks. The reason is likely to avoid a potential JNI/JNR limitation or to yield to allow GC? Possibly it's to reduce overhead of copying a large memory region in one go if JNR's get has overhead proportional to length or to avoid large allocation inside native. But copying in one go should be fine; maybe they saw performance issues and found chunking beneficial, or ensure that if buffer is huge but packet small, not copying more than needed (but they already track caplen).
It might also help CPU cache by copying piecewise, though it's questionable. It might be more for not exhausting libffi internal direct buffer length limits or something.
Now buf contains the packet data (or at least the captured portion, which is caplen bytes).
Calls cb.onPacket(tsMicros, buf, caplen).
Returns true to indicate the loop can continue.
If n == 0:
This indicates a timeout (no packet in that interval if live capture).
They return true as well (meaning just try again; effectively a silent loop).
So the capture loop will loop on timeouts doing nothing until a packet arrives or external break.
If n == -1:
An error occurred. They throw new PcapException("pcap_next_ex: " + p.pcap_geterr(ph)).
If n == -2:
This is EOF (for offline file or if live went into break state), indicating no more packets.
They return false to break the loop gracefully.
close():
Calls p.pcap_close(ph).
Possibly free any allocated memory (though hdr_pp and data_pp they might rely on GC). But they may not explicitly free those pointers; not critical as process ending frees them anyway, but ideally they would if reused. Given one handle’s close likely is end-of-lifecycle, small memory leak of a couple pointers is negligible.
Other details: - The outer JnrPcapAdapter might also override finalize or have some protection to ensure native handles closed, but using try-with-resources on PcapHandle covers that in usage.
Performance & Memory: - They aim for zero heap allocation per packet: - Reusing byte[] via SCRATCH (only alloc when grows). - Reusing PcapPkthdr struct object H for each packet (just repointing memory). - Not creating objects for each packet (no Packet class, just using the callback directly). - The only per-packet garbage should be maybe some Autoboxing or lambda overhead if any, but they likely avoid autoboxing. The tsMicros is a long (passed as primitive), the data is byte[] reused, caplen is int. - So if callback doesn’t allocate, indeed GC pressure is minimal. - The copying of packet data is necessary to move from off-heap to on-heap memory for Java to work with it. That’s the main cost. They do it chunkwise which might break up the copy, possibly to avoid a single large native call (maybe libffi might allocate temporary memory for a large copy). - If snaplen is large (e.g., 65535) then worst-case they allocate that once if a jumbo comes and reuse. - The approach should handle line rate on 1 Gbps easily, for 10 Gbps it might be a lot for one thread if small packets (e.g., ~14M packets/s at 64 bytes each is too high for Java thread), but typical real 10 Gbps traffic has larger packets and fewer pps. They also said their older pipeline sustained 10 G by optimizing similarly, presumably this can too under favorable conditions.
Extensibility: - Could integrate other libpcap functions if needed (like pcap_breakloop or pcap_statustostr). - Right now, if one wanted to implement offline reading: we’d add in JnrLibpcap a mapping for pcap_open_offline, and perhaps new method in Pcap interface. Or just use openLive by giving a filename if libpcap allows (sometimes you can do pcap_open_offline via source name in some libpcap versions but not sure, better to do separate). - Could implement additional Pcap implementations for other tech (not via this class but separate class NapatechAdapter etc., as long as they implement Pcap and return a suitable Handle). - If needed to handle different link types, JnrPcapAdapter could query pcap_datalink and possibly incorporate an Ethernet header offset handling in decoder, but right now they assume Ethernet so not here.
External Libraries: JNR (for FFI) and libpcap. It also uses JNR’s Pointer, Memory, and the structured classes from cstruct.
TimeVal.java, PcapPkthdr.java, BpfProgramStruct.java (cstruct package)
These classes extend jnr.ffi.Struct to mirror C structures: - TimeVal: has two fields tv_sec and tv_usec (SignedLong presumably 8 bytes each if 64-bit long on system, which it usually is). - PcapPkthdr: has a TimeVal ts; and two Unsigned32 fields caplen and len. (ts is a nested Struct field). - Use: after pcap_next_ex, we map the returned pointer to this struct to easily get the timestamp seconds, micros, captured length, original length. - BpfProgramStruct: has bf_len (Unsigned32) and bf_insns (Pointer). This matches struct bpf_program in C which has u_int bf_len; struct bpf_insn *bf_insns;. - Use: to pass to pcap_compile and pcap_setfilter. libpcap fills it in compile, and requires freeing via pcap_freecode.
They are straightforward: - Constructing them via new Struct(Runtime runtime) sets up the memory layout according to the platform ABI alignment and sizing rules. - The JnrPcapAdapter likely instantiates PcapPkthdr and BpfProgramStruct when needed.
Performance & usage: - Accessing Struct fields uses getter methods (like .longValue(), .intValue()) which read from memory. JNR likely uses sun.misc.Unsafe under the hood or libffi to get those offsets. - It’s convenient and still quite fast, especially since it’s just a couple of fields. - The alternative would be to manually Memory.read(8 bytes) etc. Using Struct simplifies code.
No extensibility needed beyond reflecting any structure changes if libpcap had different fields (rarely changes). - They could have included an Errbuf struct for the error buffer but they just used a direct Memory. - All in all, these classes are only used internally by adapter.
No external libs besides JNR (which these are part of using JNR’s Struct).
SegmentIO.java
(Already covered above in detail under SegmentIO in pipe section)
SegmentRecord.java
(Already covered above in pipe section)
SegbinGrep.java
(Already covered above in tools section)
PosterMain.java
(Already covered above in poster section)
BlobStore.java
(Already covered above in poster section)
Streams.java
(Already covered above in poster section)
Util.java
(Already covered above in poster section)
